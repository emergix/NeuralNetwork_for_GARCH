# üìì Notebooks Directory

This directory contains practical implementations of Neural Network-based GARCH calibration methods. Each notebook demonstrates a complete workflow from data processing to real-world financial applications.

---

## üß† Available Notebooks

### 1. GARCH Calibration with Neural Networks

## üìä The Heston Stochastic Volatility Model

The **Heston model** is a widely used stochastic volatility model in quantitative finance, particularly for pricing derivatives. Unlike Black-Scholes, which assumes constant volatility, Heston introduces a **random process for volatility itself**, capturing market phenomena such as the volatility smile and clustering.

### üßÆ Model Dynamics

The asset price \( S_t \) and its variance \( v_t \) follow the system of stochastic differential equations:

\[
\begin{aligned}
dS_t &= \mu S_t\,dt + \sqrt{v_t} S_t\,dW_t^S \\\\
dv_t &= \kappa(\theta - v_t)\,dt + \sigma \sqrt{v_t}\,dW_t^v
\end{aligned}
\]

- \( \mu \) : drift of the asset  
- \( v_t \) : instantaneous variance  
- \( \kappa \) : rate of mean reversion  
- \( \theta \) : long-term variance  
- \( \sigma \) : volatility of volatility  
- \( W_t^S \), \( W_t^v \) : Brownian motions with correlation \( \rho \)

### üîç Key Features

- **Mean-reverting variance** captures realistic volatility behavior  
- **Closed-form solution** for European options via Fourier inversion  
- **Flexible calibration** to volatility surfaces (smile/skew)

### üìÑ Calibration Example

See the full calibration process using historical Air Liquide data:

üëâ [`README_GARCH_Calibration.md`](./README_GARCH_Calibration.md)


# GARCH(1,1) ‚Äî calibration ¬´hybride¬ª : **ANN (acov_multi)** + $$\mu$$ via WLS + MLE (Gauss & Student‚Äët)  
*(Notebook : `garch_ann_full_pipeline_v2.ipynb`, v2 ‚Äúpatches A‚ÄìC‚Äù)*

Ce notebook explore une approche **hybride** pour calibrer un **GARCH(1,1)** en combinant :

- une estimation **statistique** de la persistance $$\mu \approx \alpha_1 + \beta_1$$ √† partir de la d√©croissance des autocorr√©lations (WLS),
- une **ANN/MLP** entra√Æn√©e √† pr√©dire $$\alpha_1$$ √† partir d‚Äôun vecteur de **features de type autocovariance/autocorr√©lation multi‚Äëlags** (‚Äúacov_multi‚Äù),
- une calibration finale et des **benchmarks** via **MLE** (Gauss et Student‚Äët), avec comparaisons **NLL/AIC** + profilage de $$\nu$$ (ddl) et heatmap $$(\nu,\mu)$$.

---

## 1) Donn√©es : lecture CSV ou simulation GARCH

Le notebook commence par chercher un fichier `return.csv` ou `returns.csv`.  
- Si trouv√© : il charge la **premi√®re colonne num√©rique** comme s√©rie $$x_t$$ (rendements).  
- Sinon : il **simule** une trajectoire GARCH(1,1) (param√®tres ‚Äúvrais‚Äù par d√©faut), ce qui sert de sandbox pour valider le pipeline.

Dans tous les cas, la s√©rie est **centr√©e** (soustraction de la moyenne).

---

## 2) Utilitaires : $$\hat\gamma_n$$, WLS($$\mu$$) et log‚Äëvraisemblance (NLL)

### 2.1 Autocovariance / ‚Äúgamma_hat‚Äù
Le notebook d√©finit :
- une **autocovariance empirique** √† un lag donn√©,
- une fonction `gamma_hat_series(x, lags)` qui retourne :
  - $$\hat\gamma_n$$ (normalis√© par la variance) pour plusieurs lags,
  - $$\hat\sigma^2$$ (variance empirique).

> **Id√©e** : construire un ‚Äúempreinte‚Äù multi‚Äëlags de la s√©rie, utilis√©e comme **feature engineering** pour identifier les param√®tres GARCH.

### 2.2 Estimation de $$\mu$$ par WLS sur la d√©croissance des $$\hat\gamma_n$$
Deux variantes sont test√©es :
- `estimate_mu_wls(g, lags)` : r√©gression pond√©r√©e sur $$\log(\hat\gamma_n)$$ vs $$(n-1)$$,
- `estimate_mu_wls_alt(g, lags, T_eff)` : variante o√π les poids d√©pendent de $$T - n$$ (effet ‚Äútaille d‚Äô√©chantillon effective‚Äù).

Les formules cod√©es reviennent √† approximer une loi du type :
\[
\hat\gamma_n \approx C\,\mu^{(n-1)} \quad \Rightarrow\quad \log \hat\gamma_n \approx \log C - (n-1)\,\log\mu
\]
d‚Äôo√π $$\mu = e^{-\text{slope}}$$ apr√®s r√©gression.

> **Point cl√©** : $$\mu$$ est ensuite utilis√© comme estimation de la **persistance** $$\alpha_1+\beta_1$$, et sert √† reconstruire $$\beta_1 = \mu-\alpha_1$$.

### 2.3 NLL Gaussienne & Student‚Äët
Le notebook impl√©mente :
- `nll_gaussian(alpha0, alpha1, beta1, x)`  
- `nll_student(alpha0, alpha1, beta1, nu, x)`

en imposant des contraintes de validit√© :
\[
\alpha_0>0,\; \alpha_1\ge 0,\; \beta_1\ge 0,\; \alpha_1+\beta_1 < 1
\]

---

## 3) Lags / features : ‚Äúacov_multi‚Äù (+ log‚Äëvariance)

Le vecteur de features est construit √† partir d‚Äôun ensemble de lags (par d√©faut `lags = 3..16`) :

- $$g = (\hat\gamma_{3}, \ldots, \hat\gamma_{16})$$
- et un terme suppl√©mentaire : $$\log(\hat\sigma^2)$$

> **Id√©e test√©e** : la structure multi‚Äëlags + niveau de variance contient assez d‚Äôinformation pour ‚Äúdeviner‚Äù $$\alpha_1$$ (et indirectement $$\beta_1$$ via $$\mu$$).

---

## 4) Proxy $$\alpha_1$$ robuste (Patch C) : ‚Äúlocal MLE‚Äù multi‚Äëfen√™tres / multi‚Äëguesses

Pour entra√Æner l‚ÄôANN, il faut une cible $$y$$. Ici, $$y$$ est un **proxy** de $$\alpha_1$$ obtenu **localement** sur des sous‚Äëfen√™tres.

La fonction `estimate_alpha1_local_mle_robust(sub)` :
1. calcule $$\mu$$ via WLS sur **plusieurs sets de lags** (‚Äúmulti‚Äëfen√™tres‚Äù c√¥t√© lags),
2. lance plusieurs initialisations `guesses` pour $$\alpha_1$$,
3. pour chaque essai, impose $$\beta_1 = \mu - \alpha_1$$,
4. fixe $$\alpha_0 \approx \hat\sigma^2(1-\mu)$$ (reconstruction depuis la variance),
5. minimise une NLL gaussienne locale.

> **Id√©e test√©e** : produire un $$\alpha_1$$ proxy **stable** (moins sensible aux minima locaux et au bruit) gr√¢ce √† :
- **multi‚Äëlags** (deux grilles de lags),
- **multi‚Äëstarts** (plusieurs points initiaux),
- et s√©lection du meilleur optimum (NLL minimale).

---

## 5) Dataset : fen√™tres longues + pas dense + winsorisation (Patches A/B)

Le dataset d‚Äôapprentissage est construit par glissement de fen√™tre via :
- `win = 768` (fen√™tres **longues**),
- `step = 16` (pas **dense**).

Pour chaque fen√™tre, on construit :
- les features (gamma multi‚Äëlags + log variance),
- la cible $$\alpha_1$$ via le proxy ‚Äúlocal MLE robuste‚Äù.

Ensuite, le notebook applique une **winsorisation** sur la cible :
- filtration entre les quantiles 1% et 99% sur $$y$$.

> **Id√©es test√©es** :
- plus de donn√©es (pas dense) tout en gardant une estimation locale plus ‚Äúfiable‚Äù (fen√™tres longues),
- limiter l‚Äôimpact des valeurs extr√™mes de proxy (winsorisation), qui peuvent d√©stabiliser l‚Äôentra√Ænement.

---

## 6) Split / standardisation / entra√Ænement ANN (MSE + l√©ger dropout)

### 6.1 Split + standardisation
- train/val split (80/20),
- standardisation des features : $$X \leftarrow (X-\mu_X)/\sigma_X$$.

### 6.2 Mod√®le
MLP simple (PyTorch) :
- 2 couches cach√©es (taille ~192),
- activations ReLU,
- **Dropout l√©ger** (p=0.02),
- sortie `Sigmoid()` pour borner $$\hat\alpha_1\in(0,1)$$.

### 6.3 Optimisation
- AdamW,
- scheduler `ReduceLROnPlateau`,
- early stopping (dans l‚Äôesprit : arr√™ter quand la val n‚Äôam√©liore plus).

> **Id√©e test√©e** : une ANN ‚Äúpetite mais robuste‚Äù (r√©gularis√©e) suffit √† apprendre la correspondance  
features $$\rightarrow$$ $$\alpha_1$$ proxy.

---

## 7) √âvaluation : MSE / R¬≤ + calibration finale de $$\alpha_1$$

Le notebook √©value sur la validation :
- MSE, RMSE, R¬≤,
- scatter ‚Äúvrai proxy vs pr√©dit‚Äù (calibration plot),
- histogramme des r√©sidus.

Puis il applique une **calibration** sur $$\alpha_1$$ pr√©dit :
- **Isotonic Regression** (si dispo),
- sinon fallback **lin√©aire**.

> **Id√©e test√©e** : m√™me si le MLP approxime bien la cible, une calibration monotone (isotone) peut corriger des biais syst√©matiques (compression/√©tirement).

---

## 8) Reconstruction des param√®tres GARCH finaux √† partir de l‚ÄôANN

Une fois entra√Æn√©, le notebook calcule les param√®tres ‚Äúglobaux‚Äù :

1. construire la feature globale sur toute la s√©rie (gamma multi‚Äëlags + log variance),
2. pr√©dire $$\alpha_1$$ puis calibrer : $$\hat\alpha_1$$,
3. estimer $$\hat\mu$$ via WLS alt,
4. reconstruire :
\[
\hat\beta_1 = \hat\mu - \hat\alpha_1,
\qquad
\hat\alpha_0 = \hat\sigma^2\,(1-\hat\mu)
\]
avec clipping pour respecter les contraintes (positivit√© et stationnarit√©).

---

## 9) MLE (Gauss & Student‚Äët) : baseline ‚Äúclassique‚Äù

Le notebook calcule ensuite :
- MLE gaussien ($$\alpha_0,\alpha_1,\beta_1$$),
- MLE Student‚Äët ($$\alpha_0,\alpha_1,\beta_1,\nu$$) initialis√© par la solution gaussienne.

Objectif : obtenir une baseline **optimale en vraisemblance** sous ces hypoth√®ses.

---

## 10) Benchmark ANN vs MLE : NLL/AIC + profil $$\nu$$ + heatmap $$(\nu,\mu)$$

### 10.1 Comparaison NLL & AIC
Le notebook √©value :
- NLL gaussienne pour les param√®tres ANN,
- NLL Student‚Äët pour les param√®tres ANN :
  - avec $$\nu$$ **fix√©** au $$\nu$$ du MLE‚Äët,
  - avec $$\nu$$ **profil√©** (minimisation sur une grille $$\nu\in[2.2,100]$$).

Puis il construit un tableau r√©capitulatif (export CSV + HTML) avec :
- param√®tres,
- NLL,
- AIC (via `aic_from_nll(NLL, k)` o√π $$k$$ est le nombre de param√®tres).

### 10.2 Profil de $$\nu$$
Courbe : $$\nu \mapsto \text{NLL}_t(\text{params ANN},\nu)$$  
‚Üí extraction de $$\nu^*$$ minimisant la NLL.

### 10.3 Heatmap $$(\nu,\mu)$$
Le notebook calcule une carte :
- axe x : $$\mu = \alpha_1+\beta_1$$,
- axe y : $$\nu$$,
- couleur : NLL Student‚Äët

autour de la solution ANN, et compare visuellement avec le point MLE.

> **Id√©e test√©e** : diagnostiquer la sensibilit√© de la vraisemblance Student‚Äët aux degr√©s de libert√© $$\nu$$ et √† la persistance $$\mu$$, et v√©rifier si la solution ANN ‚Äútombe‚Äù dans une vall√©e de NLL comparable √† la MLE.

---

## Lecture ‚Äúconceptuelle‚Äù du notebook

En r√©sum√©, les id√©es principales test√©es sont :

1. **Feature engineering** via signature multi‚Äëlags (acov/ACF) + niveau de variance.
2. **D√©composition du probl√®me** :  
   - estimer $$\mu$$ par une m√©thode simple (WLS),  
   - apprendre $$\alpha_1$$ par ANN,  
   - d√©duire $$\beta_1$$ puis $$\alpha_0$$.
3. **Proxy robuste** de $$\alpha_1$$ par local‚ÄëMLE multi‚Äëstarts (Patch C) pour g√©n√©rer un dataset d‚Äôentra√Ænement stable.
4. **Data augmentation temporelle** : fen√™tres longues + pas dense (Patches A/B), puis winsorisation.
5. **Calibration finale** (isotone) pour corriger les biais du mod√®le.
6. **Validation statistique** : comparaison NLL/AIC vs MLE, exploration de $$\nu$$ et des interactions $$(\nu,\mu)$$.

---

### Fichiers produits par le notebook (exports)
- `garch_ann_student_t_eval_summary.csv`
- `garch_ann_student_t_eval_summary.html`

*(si les cellules d‚Äôexport sont ex√©cut√©es)*  



[`garch_ann_full_pipeline_v2.ipynb`](./garch_ann_full_pipeline_v2.ipynb)  
*End-to-end workflow for calibrating GARCH parameters using neural networks*

Key features:
- Data preprocessing for financial time series
- Neural network architecture design (LSTM/GRU)
- Model training and validation
- Real-time calibration on streaming data
- Performance benchmarking vs traditional methods

```python
# Core calibration workflow
import tensorflow as tf
from arch import arch_model

# Neural network calibration
nn_model = tf.keras.Sequential([...])
garch_params = nn_model.predict(streaming_data)

# Feed to stochastic model
heston_model.calibrate(initial_params=garch_params[['VL','persistence']])

# Portfolio optimization
optimizer.run(volatility_forecast=garch_params['conditional_volatility'])
