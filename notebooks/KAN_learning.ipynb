{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af8d1460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KAN PyTorch + régularisation de lissage des splines (et option sparsity)\n",
    "# Idée: pénaliser la \"courbure discrète\" des coefficients spline:\n",
    "#   smooth = Σ (c_{k+1} - 2 c_k + c_{k-1})^2\n",
    "# Option: L1 sur les amplitudes (encourage sparsité des arêtes)\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Utils: B-splines 1D (Cox–de Boor)\n",
    "# -----------------------------\n",
    "def make_open_uniform_knots(n_basis: int, degree: int, xmin: float, xmax: float, device):\n",
    "    assert n_basis >= degree + 1\n",
    "    n_knots = n_basis + degree + 1\n",
    "    n_inner = n_knots - 2 * (degree + 1)\n",
    "    if n_inner > 0:\n",
    "        inner = torch.linspace(xmin, xmax, steps=n_inner + 2, device=device)[1:-1]\n",
    "        knots = torch.cat([\n",
    "            torch.full((degree + 1,), xmin, device=device),\n",
    "            inner,\n",
    "            torch.full((degree + 1,), xmax, device=device),\n",
    "        ])\n",
    "    else:\n",
    "        knots = torch.cat([\n",
    "            torch.full((degree + 1,), xmin, device=device),\n",
    "            torch.full((degree + 1,), xmax, device=device),\n",
    "        ])\n",
    "    return knots\n",
    "\n",
    "\n",
    "def bspline_basis(x: torch.Tensor, knots: torch.Tensor, degree: int, n_basis: int):\n",
    "    x = x.reshape(-1)  # [B]\n",
    "    B = x.shape[0]\n",
    "    t = knots\n",
    "    device = x.device\n",
    "\n",
    "    # degree 0\n",
    "    N = []\n",
    "    for i in range(n_basis):\n",
    "        left, right = t[i], t[i + 1]\n",
    "        cond = (x >= left) & (x < right)\n",
    "        N.append(cond.to(x.dtype))\n",
    "    N = torch.stack(N, dim=1)  # [B, n_basis]\n",
    "\n",
    "    xmax = t[-1]\n",
    "    at_right = (x == xmax)\n",
    "    if at_right.any():\n",
    "        N[at_right, :] = 0\n",
    "        N[at_right, -1] = 1\n",
    "\n",
    "    # recursion\n",
    "    for d in range(1, degree + 1):\n",
    "        N_new = torch.zeros((B, n_basis), device=device, dtype=x.dtype)\n",
    "        for i in range(n_basis):\n",
    "            denom1 = t[i + d] - t[i]\n",
    "            if denom1 != 0:\n",
    "                a = (x - t[i]) / denom1\n",
    "                left_term = a * N[:, i]\n",
    "            else:\n",
    "                left_term = 0.0\n",
    "\n",
    "            denom2 = t[i + d + 1] - t[i + 1]\n",
    "            if denom2 != 0 and i + 1 < n_basis:\n",
    "                b = (t[i + d + 1] - x) / denom2\n",
    "                right_term = b * N[:, i + 1]\n",
    "            else:\n",
    "                right_term = 0.0\n",
    "\n",
    "            N_new[:, i] = left_term + right_term\n",
    "        N = N_new\n",
    "    return N\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# KAN Layer\n",
    "# -----------------------------\n",
    "class KANSplineLayer(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int,\n",
    "                 n_basis: int = 16, degree: int = 3,\n",
    "                 xmin: float = -1.0, xmax: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.n_basis = n_basis\n",
    "        self.degree = degree\n",
    "        self.xmin = float(xmin)\n",
    "        self.xmax = float(xmax)\n",
    "\n",
    "        self.coeff = nn.Parameter(0.01 * torch.randn(out_dim, in_dim, n_basis))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_dim))\n",
    "\n",
    "        knots = make_open_uniform_knots(n_basis, degree, self.xmin, self.xmax, device=torch.device(\"cpu\"))\n",
    "        self.register_buffer(\"knots\", knots)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        B, D = x.shape\n",
    "        assert D == self.in_dim\n",
    "        x = torch.clamp(x, self.xmin, self.xmax)\n",
    "\n",
    "        out = torch.zeros((B, self.out_dim), device=x.device, dtype=x.dtype)\n",
    "        for j in range(self.in_dim):\n",
    "            basis = bspline_basis(x[:, j], self.knots, self.degree, self.n_basis)  # [B, K]\n",
    "            out += basis @ self.coeff[:, j, :].transpose(0, 1)  # [B, out]\n",
    "        return out + self.bias\n",
    "\n",
    "\n",
    "class KAN(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int,\n",
    "                 n_basis: int = 16, degree: int = 3,\n",
    "                 xmin: float = -1.0, xmax: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.l1 = KANSplineLayer(in_dim, hidden_dim, n_basis, degree, xmin, xmax)\n",
    "        self.l2 = KANSplineLayer(hidden_dim, out_dim, n_basis, degree, xmin, xmax)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.tanh(self.l1(x))\n",
    "        return self.l2(h)\n",
    "\n",
    "    def spline_smoothness_penalty(self):\n",
    "        \"\"\"\n",
    "        Penalise discrete second derivative of spline coefficients.\n",
    "        Returns scalar tensor.\n",
    "        \"\"\"\n",
    "        penalty = 0.0\n",
    "        for layer in (self.l1, self.l2):\n",
    "            c = layer.coeff  # [out, in, K]\n",
    "            # second difference along K: c[k+1] - 2c[k] + c[k-1]\n",
    "            d2 = c[..., 2:] - 2.0 * c[..., 1:-1] + c[..., :-2]  # [out, in, K-2]\n",
    "            penalty = penalty + (d2 ** 2).mean()\n",
    "        return penalty\n",
    "\n",
    "    def l1_amplitude_penalty(self):\n",
    "        \"\"\"\n",
    "        Optional: encourage sparse/small functions on edges (like \"edge pruning\").\n",
    "        \"\"\"\n",
    "        pen = 0.0\n",
    "        for layer in (self.l1, self.l2):\n",
    "            pen = pen + layer.coeff.abs().mean()\n",
    "        return pen\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset multivarié synthétique\n",
    "# -----------------------------\n",
    "def make_dataset(n=30000, in_dim=6, noise=0.05, seed=42, device=\"cpu\"):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    X = (torch.rand((n, in_dim), device=device) * 2 - 1)  # [-1,1]\n",
    "\n",
    "    y = (\n",
    "        torch.sin(math.pi * X[:, 0]) +\n",
    "        0.5 * X[:, 1] ** 2 -\n",
    "        X[:, 2] * X[:, 3] +\n",
    "        torch.exp(0.7 * X[:, 4]) -\n",
    "        0.3 * torch.cos(3.0 * X[:, 5])\n",
    "    ).unsqueeze(1)\n",
    "\n",
    "    y = y + noise * torch.randn(y.shape, device=device)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Train (avec lissage + clipping)\n",
    "# -----------------------------\n",
    "def train_kan(device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    in_dim = 6\n",
    "    X, y = make_dataset(n=30000, in_dim=in_dim, noise=0.05, seed=42, device=device)\n",
    "\n",
    "    n_train = int(0.85 * X.shape[0])\n",
    "    X_train, y_train = X[:n_train], y[:n_train]\n",
    "    X_val, y_val = X[n_train:], y[n_train:]\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=512, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=2048, shuffle=False)\n",
    "\n",
    "    model = KAN(in_dim=in_dim, hidden_dim=32, out_dim=1, n_basis=24, degree=3, xmin=-1.0, xmax=1.0).to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=8e-4, weight_decay=1e-4)\n",
    "\n",
    "    # Hyperparams régularisation\n",
    "    lambda_smooth = 1e-2   # lissage (augmente si ça \"vibre\", diminue si sous-fit)\n",
    "    lambda_l1 = 0.0        # mets 1e-4 ou 1e-5 si tu veux sparsifier\n",
    "\n",
    "    def eval_mse():\n",
    "        model.eval()\n",
    "        mse_sum = 0.0\n",
    "        n = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                pred = model(xb)\n",
    "                mse_sum += F.mse_loss(pred, yb, reduction=\"sum\").item()\n",
    "                n += yb.numel()\n",
    "        return mse_sum / n\n",
    "    \n",
    "    best_val = float(\"inf\")\n",
    "\n",
    "\n",
    "    for epoch in range(1, 31):\n",
    "        model.train()\n",
    "        train_mse_sum = 0.0\n",
    "        n_seen = 0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            pred = model(xb)\n",
    "            mse = F.mse_loss(pred, yb)\n",
    "\n",
    "            smooth = model.spline_smoothness_penalty()\n",
    "            l1 = model.l1_amplitude_penalty() if lambda_l1 > 0 else 0.0\n",
    "\n",
    "            loss = mse + lambda_smooth * smooth + (lambda_l1 * l1 if lambda_l1 > 0 else 0.0)\n",
    "            loss.backward()\n",
    "\n",
    "            # Stabilise gradients (très utile sur KAN)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            opt.step()\n",
    "\n",
    "            train_mse_sum += mse.item() * yb.shape[0]\n",
    "            n_seen += yb.shape[0]\n",
    "\n",
    "        train_mse = train_mse_sum / n_seen\n",
    "        val_mse = eval_mse()\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            s = float(model.spline_smoothness_penalty().detach().cpu())\n",
    "            print(f\"epoch {epoch:02d} | train MSE={train_mse:.6f} | val MSE={val_mse:.6f} | smooth={s:.6f}\")\n",
    "            \n",
    "       \n",
    "        \n",
    "          # Sauvegarde si meilleur modèle\n",
    "        if val_mse < best_val:\n",
    "            best_val = val_mse\n",
    "            torch.save(model.state_dict(), \"kan_model.pt\")\n",
    "\n",
    "        print(f\"epoch {epoch} | val MSE={val_mse:.6f} | best={best_val:.6f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "af5232a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Comptage de paramètres\n",
    "# -----------------------------\n",
    "def count_params(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) MLP \"flexible\" (N hidden layers)\n",
    "# -----------------------------\n",
    "class FlexibleMLP(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dims: list[int], out_dim: int, act: str = \"tanh\"):\n",
    "        super().__init__()\n",
    "        acts = {\n",
    "            \"tanh\": nn.Tanh(),\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"gelu\": nn.GELU(),\n",
    "            \"silu\": nn.SiLU(),\n",
    "        }\n",
    "        if act not in acts:\n",
    "            raise ValueError(f\"Unknown act={act}. Choose from {list(acts.keys())}\")\n",
    "\n",
    "        layers = []\n",
    "        prev = in_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(acts[act])\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, out_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Estimation du nb de params d'un MLP\n",
    "# -----------------------------\n",
    "def mlp_param_count(in_dim: int, hidden_dims: list[int], out_dim: int) -> int:\n",
    "    # Chaque Linear: weights + bias = (prev*h) + h\n",
    "    total = 0\n",
    "    prev = in_dim\n",
    "    for h in hidden_dims:\n",
    "        total += prev * h + h\n",
    "        prev = h\n",
    "    total += prev * out_dim + out_dim\n",
    "    return total\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Trouver une architecture MLP proche d'un target_params\n",
    "#    - mode \"1 hidden layer\" ou \"2 hidden layers\" (recommandé)\n",
    "# -----------------------------\n",
    "def find_mlp_dims_close_to_target(\n",
    "    target_params: int,\n",
    "    in_dim: int,\n",
    "    out_dim: int,\n",
    "    n_hidden_layers: int = 2,\n",
    "    h_min: int = 4,\n",
    "    h_max: int = 4096,\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Retourne hidden_dims (liste) qui minimise |params - target_params|.\n",
    "    - n_hidden_layers=1 : résout quasi-analytique\n",
    "    - n_hidden_layers=2 : petite recherche grille intelligente\n",
    "    \"\"\"\n",
    "    if n_hidden_layers == 1:\n",
    "        # params = (in*h + h) + (h*out + out) = h*(in+1+out) + out\n",
    "        denom = in_dim + 1 + out_dim\n",
    "        h = max(h_min, int(round((target_params - out_dim) / denom)))\n",
    "        h = min(h_max, h)\n",
    "        return [h]\n",
    "\n",
    "    if n_hidden_layers == 2:\n",
    "        # Recherche simple : h1 dans une plage, h2 déduit approximativement\n",
    "        best = None\n",
    "        best_dims = None\n",
    "\n",
    "        # On balaye h1 sur une grille log-ish (plus large)\n",
    "        # Tu peux densifier si tu veux.\n",
    "        for h1 in [int(x) for x in torch.unique(torch.logspace(math.log10(h_min), math.log10(min(2048, h_max)), steps=60)).tolist()]:\n",
    "            # params = (in*h1+h1) + (h1*h2+h2) + (h2*out+out)\n",
    "            #       = h1*(in+1) + h2*(h1+1+out) + out\n",
    "            a = h1 * (in_dim + 1) + out_dim\n",
    "            denom = (h1 + 1 + out_dim)\n",
    "            # h2 approx\n",
    "            h2 = int(round((target_params - a) / max(1, denom)))\n",
    "            if h2 < h_min or h2 > h_max:\n",
    "                continue\n",
    "\n",
    "            # On regarde autour de h2 pour ajuster finement\n",
    "            for h2_try in [h2 - 2, h2 - 1, h2, h2 + 1, h2 + 2]:\n",
    "                if h2_try < h_min or h2_try > h_max:\n",
    "                    continue\n",
    "                p = mlp_param_count(in_dim, [h1, h2_try], out_dim)\n",
    "                err = abs(p - target_params)\n",
    "                if (best is None) or (err < best):\n",
    "                    best = err\n",
    "                    best_dims = [h1, h2_try]\n",
    "\n",
    "        # Fallback si la recherche n'a rien trouvé\n",
    "        if best_dims is None:\n",
    "            # fallback 1 couche\n",
    "            return find_mlp_dims_close_to_target(target_params, in_dim, out_dim, n_hidden_layers=1, h_min=h_min, h_max=h_max)\n",
    "\n",
    "        return best_dims\n",
    "\n",
    "    raise ValueError(\"n_hidden_layers must be 1 or 2\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Entraînement MLP avec target param count\n",
    "#    - Tu fournis ton KAN déjà créé (pour récupérer son nb de params)\n",
    "# -----------------------------\n",
    "def train_mlp_matched_to_kan(\n",
    "    kan_model: nn.Module,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    act: str = \"tanh\",\n",
    "    n_hidden_layers: int = 2,\n",
    "    lr: float = 1e-3,\n",
    "    weight_decay: float = 1e-4,\n",
    "    epochs: int = 30,\n",
    "):\n",
    "    # Tu réutilises TON make_dataset existant\n",
    "    in_dim = 6\n",
    "    out_dim = 1\n",
    "\n",
    "    target_params = count_params(kan_model)\n",
    "    hidden_dims = find_mlp_dims_close_to_target(\n",
    "        target_params=target_params,\n",
    "        in_dim=in_dim,\n",
    "        out_dim=out_dim,\n",
    "        n_hidden_layers=n_hidden_layers,\n",
    "        h_min=8,\n",
    "        h_max=4096,\n",
    "    )\n",
    "\n",
    "    mlp = FlexibleMLP(in_dim=in_dim, hidden_dims=hidden_dims, out_dim=out_dim, act=act).to(device)\n",
    "    mlp_params = count_params(mlp)\n",
    "\n",
    "    print(f\"[MATCH] KAN params={target_params} | MLP hidden_dims={hidden_dims} | MLP params={mlp_params} | diff={mlp_params-target_params}\")\n",
    "\n",
    "    X, y = make_dataset(n=30000, in_dim=in_dim, noise=0.05, seed=42, device=device)\n",
    "\n",
    "    n_train = int(0.85 * X.shape[0])\n",
    "    X_train, y_train = X[:n_train], y[:n_train]\n",
    "    X_val, y_val = X[n_train:], y[n_train:]\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=512, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=2048, shuffle=False)\n",
    "\n",
    "    opt = torch.optim.AdamW(mlp.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        opt,\n",
    "        step_size=60,   # baisse toutes les 60 epochs\n",
    "        gamma=0.3       # LR = LR × 0.3\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    def eval_mse():\n",
    "        mlp.eval()\n",
    "        mse_sum, n = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                pred = mlp(xb)\n",
    "                mse_sum += F.mse_loss(pred, yb, reduction=\"sum\").item()\n",
    "                n += yb.numel()\n",
    "        return mse_sum / n\n",
    "    \n",
    "    best_val = float(\"inf\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        mlp.train()\n",
    "        train_mse_sum, n_seen = 0.0, 0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            pred = mlp(xb)\n",
    "            loss = F.mse_loss(pred, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(mlp.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "            train_mse_sum += loss.item() * yb.shape[0]\n",
    "            n_seen += yb.shape[0]\n",
    "\n",
    "        train_mse = train_mse_sum / n_seen\n",
    "        val_mse = eval_mse()\n",
    "        scheduler.step()\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(f\"[MLP-MATCH] epoch {epoch:02d} | train MSE={train_mse:.6f} | val MSE={val_mse:.6f}\")\n",
    "            \n",
    "        # Sauvegarde si meilleur modèle\n",
    "        if val_mse < best_val:\n",
    "            best_val = val_mse\n",
    "            torch.save(mlp.state_dict(), \"best_model.pt\")\n",
    "\n",
    "        print(f\"epoch {epoch} | val MSE={val_mse:.6f} | best={best_val:.6f}\")\n",
    "\n",
    "    return mlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "baef2043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os\n",
    "import torch\n",
    "\n",
    "def now_sync(device):\n",
    "    if device.startswith(\"cuda\") and torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def timed(fn, device, *args, **kwargs):\n",
    "    now_sync(device)\n",
    "    t0 = time.perf_counter()\n",
    "    out = fn(*args, **kwargs)\n",
    "    now_sync(device)\n",
    "    t1 = time.perf_counter()\n",
    "    return out, (t1 - t0)\n",
    "\n",
    "def save_best_state_dict(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_state_dict(model, path, device):\n",
    "    sd = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(sd)\n",
    "    return model\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3816daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01 | train MSE=1.158183 | val MSE=0.523067 | smooth=0.000695\n",
      "epoch 1 | val MSE=0.523067 | best=0.523067\n",
      "epoch 2 | val MSE=0.127265 | best=0.127265\n",
      "epoch 3 | val MSE=0.114624 | best=0.114624\n",
      "epoch 4 | val MSE=0.079720 | best=0.079720\n",
      "epoch 05 | train MSE=0.062709 | val MSE=0.049747 | smooth=0.001243\n",
      "epoch 5 | val MSE=0.049747 | best=0.049747\n",
      "epoch 6 | val MSE=0.028451 | best=0.028451\n",
      "epoch 7 | val MSE=0.014021 | best=0.014021\n",
      "epoch 8 | val MSE=0.007070 | best=0.007070\n",
      "epoch 9 | val MSE=0.004226 | best=0.004226\n",
      "epoch 10 | train MSE=0.003570 | val MSE=0.003346 | smooth=0.001872\n",
      "epoch 10 | val MSE=0.003346 | best=0.003346\n",
      "epoch 11 | val MSE=0.003007 | best=0.003007\n",
      "epoch 12 | val MSE=0.002910 | best=0.002910\n",
      "epoch 13 | val MSE=0.002849 | best=0.002849\n",
      "epoch 14 | val MSE=0.002852 | best=0.002849\n",
      "epoch 15 | train MSE=0.002686 | val MSE=0.002781 | smooth=0.001859\n",
      "epoch 15 | val MSE=0.002781 | best=0.002781\n",
      "epoch 16 | val MSE=0.002802 | best=0.002781\n",
      "epoch 17 | val MSE=0.002878 | best=0.002781\n",
      "epoch 18 | val MSE=0.002807 | best=0.002781\n",
      "epoch 19 | val MSE=0.002775 | best=0.002775\n",
      "epoch 20 | train MSE=0.002668 | val MSE=0.002799 | smooth=0.001743\n",
      "epoch 20 | val MSE=0.002799 | best=0.002775\n",
      "epoch 21 | val MSE=0.002745 | best=0.002745\n",
      "epoch 22 | val MSE=0.002889 | best=0.002745\n",
      "epoch 23 | val MSE=0.002793 | best=0.002745\n",
      "epoch 24 | val MSE=0.002778 | best=0.002745\n",
      "epoch 25 | train MSE=0.002664 | val MSE=0.003067 | smooth=0.001659\n",
      "epoch 25 | val MSE=0.003067 | best=0.002745\n",
      "epoch 26 | val MSE=0.002802 | best=0.002745\n",
      "epoch 27 | val MSE=0.002762 | best=0.002745\n",
      "epoch 28 | val MSE=0.002940 | best=0.002745\n",
      "epoch 29 | val MSE=0.002825 | best=0.002745\n",
      "epoch 30 | train MSE=0.002624 | val MSE=0.002762 | smooth=0.001608\n",
      "epoch 30 | val MSE=0.002762 | best=0.002745\n",
      "KAN train time: 2681.06s | params: 5409\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# KAN\n",
    "kan, t_kan = timed(lambda: train_kan(device=device), device)\n",
    "\n",
    "print(f\"KAN train time: {t_kan:.2f}s | params: {count_params(kan)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "021d3e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MATCH] KAN params=5409 | MLP hidden_dims=[9, 486] | MLP params=5410 | diff=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emerg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLP-MATCH] epoch 01 | train MSE=0.781585 | val MSE=0.439276\n",
      "epoch 1 | val MSE=0.439276 | best=0.439276\n",
      "epoch 2 | val MSE=0.362606 | best=0.362606\n",
      "epoch 3 | val MSE=0.351370 | best=0.351370\n",
      "epoch 4 | val MSE=0.335211 | best=0.335211\n",
      "[MLP-MATCH] epoch 05 | train MSE=0.331982 | val MSE=0.316070\n",
      "epoch 5 | val MSE=0.316070 | best=0.316070\n",
      "epoch 6 | val MSE=0.298737 | best=0.298737\n",
      "epoch 7 | val MSE=0.285698 | best=0.285698\n",
      "epoch 8 | val MSE=0.279792 | best=0.279792\n",
      "epoch 9 | val MSE=0.278115 | best=0.278115\n",
      "[MLP-MATCH] epoch 10 | train MSE=0.282475 | val MSE=0.276568\n",
      "epoch 10 | val MSE=0.276568 | best=0.276568\n",
      "epoch 11 | val MSE=0.274124 | best=0.274124\n",
      "epoch 12 | val MSE=0.274269 | best=0.274124\n",
      "epoch 13 | val MSE=0.272034 | best=0.272034\n",
      "epoch 14 | val MSE=0.269454 | best=0.269454\n",
      "[MLP-MATCH] epoch 15 | train MSE=0.270758 | val MSE=0.267036\n",
      "epoch 15 | val MSE=0.267036 | best=0.267036\n",
      "epoch 16 | val MSE=0.266829 | best=0.266829\n",
      "epoch 17 | val MSE=0.264764 | best=0.264764\n",
      "epoch 18 | val MSE=0.262507 | best=0.262507\n",
      "epoch 19 | val MSE=0.259857 | best=0.259857\n",
      "[MLP-MATCH] epoch 20 | train MSE=0.257882 | val MSE=0.262965\n",
      "epoch 20 | val MSE=0.262965 | best=0.259857\n",
      "epoch 21 | val MSE=0.254672 | best=0.254672\n",
      "epoch 22 | val MSE=0.246828 | best=0.246828\n",
      "epoch 23 | val MSE=0.239145 | best=0.239145\n",
      "epoch 24 | val MSE=0.233699 | best=0.233699\n",
      "[MLP-MATCH] epoch 25 | train MSE=0.230073 | val MSE=0.230304\n",
      "epoch 25 | val MSE=0.230304 | best=0.230304\n",
      "epoch 26 | val MSE=0.226987 | best=0.226987\n",
      "epoch 27 | val MSE=0.223425 | best=0.223425\n",
      "epoch 28 | val MSE=0.221913 | best=0.221913\n",
      "epoch 29 | val MSE=0.217706 | best=0.217706\n",
      "[MLP-MATCH] epoch 30 | train MSE=0.215921 | val MSE=0.214662\n",
      "epoch 30 | val MSE=0.214662 | best=0.214662\n",
      "epoch 31 | val MSE=0.206460 | best=0.206460\n",
      "epoch 32 | val MSE=0.199347 | best=0.199347\n",
      "epoch 33 | val MSE=0.184439 | best=0.184439\n",
      "epoch 34 | val MSE=0.163982 | best=0.163982\n",
      "[MLP-MATCH] epoch 35 | train MSE=0.154402 | val MSE=0.140889\n",
      "epoch 35 | val MSE=0.140889 | best=0.140889\n",
      "epoch 36 | val MSE=0.108490 | best=0.108490\n",
      "epoch 37 | val MSE=0.078554 | best=0.078554\n",
      "epoch 38 | val MSE=0.057382 | best=0.057382\n",
      "epoch 39 | val MSE=0.047683 | best=0.047683\n",
      "[MLP-MATCH] epoch 40 | train MSE=0.045136 | val MSE=0.041391\n",
      "epoch 40 | val MSE=0.041391 | best=0.041391\n",
      "epoch 41 | val MSE=0.038815 | best=0.038815\n",
      "epoch 42 | val MSE=0.037793 | best=0.037793\n",
      "epoch 43 | val MSE=0.037261 | best=0.037261\n",
      "epoch 44 | val MSE=0.036604 | best=0.036604\n",
      "[MLP-MATCH] epoch 45 | train MSE=0.036649 | val MSE=0.035241\n",
      "epoch 45 | val MSE=0.035241 | best=0.035241\n",
      "epoch 46 | val MSE=0.033796 | best=0.033796\n",
      "epoch 47 | val MSE=0.033273 | best=0.033273\n",
      "epoch 48 | val MSE=0.032210 | best=0.032210\n",
      "epoch 49 | val MSE=0.032695 | best=0.032210\n",
      "[MLP-MATCH] epoch 50 | train MSE=0.031640 | val MSE=0.031489\n",
      "epoch 50 | val MSE=0.031489 | best=0.031489\n",
      "epoch 51 | val MSE=0.029287 | best=0.029287\n",
      "epoch 52 | val MSE=0.027803 | best=0.027803\n",
      "epoch 53 | val MSE=0.025986 | best=0.025986\n",
      "epoch 54 | val MSE=0.023800 | best=0.023800\n",
      "[MLP-MATCH] epoch 55 | train MSE=0.023105 | val MSE=0.021403\n",
      "epoch 55 | val MSE=0.021403 | best=0.021403\n",
      "epoch 56 | val MSE=0.017844 | best=0.017844\n",
      "epoch 57 | val MSE=0.015116 | best=0.015116\n",
      "epoch 58 | val MSE=0.012583 | best=0.012583\n",
      "epoch 59 | val MSE=0.011210 | best=0.011210\n",
      "[MLP-MATCH] epoch 60 | train MSE=0.011352 | val MSE=0.010786\n",
      "epoch 60 | val MSE=0.010786 | best=0.010786\n",
      "epoch 61 | val MSE=0.010305 | best=0.010305\n",
      "epoch 62 | val MSE=0.009818 | best=0.009818\n",
      "epoch 63 | val MSE=0.009230 | best=0.009230\n",
      "epoch 64 | val MSE=0.009179 | best=0.009179\n",
      "[MLP-MATCH] epoch 65 | train MSE=0.009270 | val MSE=0.008971\n",
      "epoch 65 | val MSE=0.008971 | best=0.008971\n",
      "epoch 66 | val MSE=0.008886 | best=0.008886\n",
      "epoch 67 | val MSE=0.008818 | best=0.008818\n",
      "epoch 68 | val MSE=0.008290 | best=0.008290\n",
      "epoch 69 | val MSE=0.008165 | best=0.008165\n",
      "[MLP-MATCH] epoch 70 | train MSE=0.008258 | val MSE=0.008854\n",
      "epoch 70 | val MSE=0.008854 | best=0.008165\n",
      "epoch 71 | val MSE=0.007975 | best=0.007975\n",
      "epoch 72 | val MSE=0.007835 | best=0.007835\n",
      "epoch 73 | val MSE=0.007523 | best=0.007523\n",
      "epoch 74 | val MSE=0.007420 | best=0.007420\n",
      "[MLP-MATCH] epoch 75 | train MSE=0.007651 | val MSE=0.007582\n",
      "epoch 75 | val MSE=0.007582 | best=0.007420\n",
      "epoch 76 | val MSE=0.007468 | best=0.007420\n",
      "epoch 77 | val MSE=0.007210 | best=0.007210\n",
      "epoch 78 | val MSE=0.007795 | best=0.007210\n",
      "epoch 79 | val MSE=0.007147 | best=0.007147\n",
      "[MLP-MATCH] epoch 80 | train MSE=0.007339 | val MSE=0.007616\n",
      "epoch 80 | val MSE=0.007616 | best=0.007147\n",
      "epoch 81 | val MSE=0.006869 | best=0.006869\n",
      "epoch 82 | val MSE=0.006719 | best=0.006719\n",
      "epoch 83 | val MSE=0.006786 | best=0.006719\n",
      "epoch 84 | val MSE=0.006585 | best=0.006585\n",
      "[MLP-MATCH] epoch 85 | train MSE=0.006661 | val MSE=0.007128\n",
      "epoch 85 | val MSE=0.007128 | best=0.006585\n",
      "epoch 86 | val MSE=0.006540 | best=0.006540\n",
      "epoch 87 | val MSE=0.008091 | best=0.006540\n",
      "epoch 88 | val MSE=0.006269 | best=0.006269\n",
      "epoch 89 | val MSE=0.006517 | best=0.006269\n",
      "[MLP-MATCH] epoch 90 | train MSE=0.006731 | val MSE=0.006183\n",
      "epoch 90 | val MSE=0.006183 | best=0.006183\n",
      "epoch 91 | val MSE=0.006852 | best=0.006183\n",
      "epoch 92 | val MSE=0.007159 | best=0.006183\n",
      "epoch 93 | val MSE=0.006203 | best=0.006183\n",
      "epoch 94 | val MSE=0.006402 | best=0.006183\n",
      "[MLP-MATCH] epoch 95 | train MSE=0.006257 | val MSE=0.006124\n",
      "epoch 95 | val MSE=0.006124 | best=0.006124\n",
      "epoch 96 | val MSE=0.005848 | best=0.005848\n",
      "epoch 97 | val MSE=0.005986 | best=0.005848\n",
      "epoch 98 | val MSE=0.005788 | best=0.005788\n",
      "epoch 99 | val MSE=0.005828 | best=0.005788\n",
      "[MLP-MATCH] epoch 100 | train MSE=0.006073 | val MSE=0.005794\n",
      "epoch 100 | val MSE=0.005794 | best=0.005788\n",
      "epoch 101 | val MSE=0.006014 | best=0.005788\n",
      "epoch 102 | val MSE=0.005873 | best=0.005788\n",
      "epoch 103 | val MSE=0.005633 | best=0.005633\n",
      "epoch 104 | val MSE=0.005579 | best=0.005579\n",
      "[MLP-MATCH] epoch 105 | train MSE=0.005560 | val MSE=0.005728\n",
      "epoch 105 | val MSE=0.005728 | best=0.005579\n",
      "epoch 106 | val MSE=0.005893 | best=0.005579\n",
      "epoch 107 | val MSE=0.005432 | best=0.005432\n",
      "epoch 108 | val MSE=0.006467 | best=0.005432\n",
      "epoch 109 | val MSE=0.006186 | best=0.005432\n",
      "[MLP-MATCH] epoch 110 | train MSE=0.005563 | val MSE=0.005616\n",
      "epoch 110 | val MSE=0.005616 | best=0.005432\n",
      "epoch 111 | val MSE=0.005596 | best=0.005432\n",
      "epoch 112 | val MSE=0.005303 | best=0.005303\n",
      "epoch 113 | val MSE=0.005454 | best=0.005303\n",
      "epoch 114 | val MSE=0.005473 | best=0.005303\n",
      "[MLP-MATCH] epoch 115 | train MSE=0.005364 | val MSE=0.005565\n",
      "epoch 115 | val MSE=0.005565 | best=0.005303\n",
      "epoch 116 | val MSE=0.005148 | best=0.005148\n",
      "epoch 117 | val MSE=0.005463 | best=0.005148\n",
      "epoch 118 | val MSE=0.004995 | best=0.004995\n",
      "epoch 119 | val MSE=0.005284 | best=0.004995\n",
      "[MLP-MATCH] epoch 120 | train MSE=0.005207 | val MSE=0.005140\n",
      "epoch 120 | val MSE=0.005140 | best=0.004995\n",
      "epoch 121 | val MSE=0.005134 | best=0.004995\n",
      "epoch 122 | val MSE=0.005269 | best=0.004995\n",
      "epoch 123 | val MSE=0.005361 | best=0.004995\n",
      "epoch 124 | val MSE=0.004971 | best=0.004971\n",
      "[MLP-MATCH] epoch 125 | train MSE=0.004994 | val MSE=0.005207\n",
      "epoch 125 | val MSE=0.005207 | best=0.004971\n",
      "epoch 126 | val MSE=0.004737 | best=0.004737\n",
      "epoch 127 | val MSE=0.004793 | best=0.004737\n",
      "epoch 128 | val MSE=0.004702 | best=0.004702\n",
      "epoch 129 | val MSE=0.005053 | best=0.004702\n",
      "[MLP-MATCH] epoch 130 | train MSE=0.004820 | val MSE=0.004539\n",
      "epoch 130 | val MSE=0.004539 | best=0.004539\n",
      "epoch 131 | val MSE=0.004838 | best=0.004539\n",
      "epoch 132 | val MSE=0.004753 | best=0.004539\n",
      "epoch 133 | val MSE=0.004423 | best=0.004423\n",
      "epoch 134 | val MSE=0.004882 | best=0.004423\n",
      "[MLP-MATCH] epoch 135 | train MSE=0.004749 | val MSE=0.004333\n",
      "epoch 135 | val MSE=0.004333 | best=0.004333\n",
      "epoch 136 | val MSE=0.004722 | best=0.004333\n",
      "epoch 137 | val MSE=0.004481 | best=0.004333\n",
      "epoch 138 | val MSE=0.004320 | best=0.004320\n",
      "epoch 139 | val MSE=0.004424 | best=0.004320\n",
      "[MLP-MATCH] epoch 140 | train MSE=0.004570 | val MSE=0.004267\n",
      "epoch 140 | val MSE=0.004267 | best=0.004267\n",
      "epoch 141 | val MSE=0.004161 | best=0.004161\n",
      "epoch 142 | val MSE=0.004328 | best=0.004161\n",
      "epoch 143 | val MSE=0.004051 | best=0.004051\n",
      "epoch 144 | val MSE=0.004472 | best=0.004051\n",
      "[MLP-MATCH] epoch 145 | train MSE=0.004346 | val MSE=0.004473\n",
      "epoch 145 | val MSE=0.004473 | best=0.004051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 146 | val MSE=0.003997 | best=0.003997\n",
      "epoch 147 | val MSE=0.004207 | best=0.003997\n",
      "epoch 148 | val MSE=0.004118 | best=0.003997\n",
      "epoch 149 | val MSE=0.004038 | best=0.003997\n",
      "[MLP-MATCH] epoch 150 | train MSE=0.004305 | val MSE=0.004101\n",
      "epoch 150 | val MSE=0.004101 | best=0.003997\n",
      "epoch 151 | val MSE=0.004141 | best=0.003997\n",
      "epoch 152 | val MSE=0.004751 | best=0.003997\n",
      "epoch 153 | val MSE=0.004192 | best=0.003997\n",
      "epoch 154 | val MSE=0.003841 | best=0.003841\n",
      "[MLP-MATCH] epoch 155 | train MSE=0.004203 | val MSE=0.003877\n",
      "epoch 155 | val MSE=0.003877 | best=0.003841\n",
      "epoch 156 | val MSE=0.003790 | best=0.003790\n",
      "epoch 157 | val MSE=0.004164 | best=0.003790\n",
      "epoch 158 | val MSE=0.003715 | best=0.003715\n",
      "epoch 159 | val MSE=0.003962 | best=0.003715\n",
      "[MLP-MATCH] epoch 160 | train MSE=0.003860 | val MSE=0.003721\n",
      "epoch 160 | val MSE=0.003721 | best=0.003715\n",
      "epoch 161 | val MSE=0.004084 | best=0.003715\n",
      "epoch 162 | val MSE=0.003668 | best=0.003668\n",
      "epoch 163 | val MSE=0.003676 | best=0.003668\n",
      "epoch 164 | val MSE=0.003844 | best=0.003668\n",
      "[MLP-MATCH] epoch 165 | train MSE=0.003911 | val MSE=0.004179\n",
      "epoch 165 | val MSE=0.004179 | best=0.003668\n",
      "epoch 166 | val MSE=0.003866 | best=0.003668\n",
      "epoch 167 | val MSE=0.003656 | best=0.003656\n",
      "epoch 168 | val MSE=0.003737 | best=0.003656\n",
      "epoch 169 | val MSE=0.003738 | best=0.003656\n",
      "[MLP-MATCH] epoch 170 | train MSE=0.003979 | val MSE=0.004105\n",
      "epoch 170 | val MSE=0.004105 | best=0.003656\n",
      "epoch 171 | val MSE=0.003645 | best=0.003645\n",
      "epoch 172 | val MSE=0.003995 | best=0.003645\n",
      "epoch 173 | val MSE=0.003996 | best=0.003645\n",
      "epoch 174 | val MSE=0.003700 | best=0.003645\n",
      "[MLP-MATCH] epoch 175 | train MSE=0.004049 | val MSE=0.005214\n",
      "epoch 175 | val MSE=0.005214 | best=0.003645\n",
      "epoch 176 | val MSE=0.004120 | best=0.003645\n",
      "epoch 177 | val MSE=0.003697 | best=0.003645\n",
      "epoch 178 | val MSE=0.003526 | best=0.003526\n",
      "epoch 179 | val MSE=0.004530 | best=0.003526\n",
      "[MLP-MATCH] epoch 180 | train MSE=0.003749 | val MSE=0.003537\n",
      "epoch 180 | val MSE=0.003537 | best=0.003526\n",
      "epoch 181 | val MSE=0.003571 | best=0.003526\n",
      "epoch 182 | val MSE=0.003514 | best=0.003514\n",
      "epoch 183 | val MSE=0.003577 | best=0.003514\n",
      "epoch 184 | val MSE=0.003451 | best=0.003451\n",
      "[MLP-MATCH] epoch 185 | train MSE=0.003562 | val MSE=0.003402\n",
      "epoch 185 | val MSE=0.003402 | best=0.003402\n",
      "epoch 186 | val MSE=0.003423 | best=0.003402\n",
      "epoch 187 | val MSE=0.003472 | best=0.003402\n",
      "epoch 188 | val MSE=0.003444 | best=0.003402\n",
      "epoch 189 | val MSE=0.003405 | best=0.003402\n",
      "[MLP-MATCH] epoch 190 | train MSE=0.003638 | val MSE=0.003380\n",
      "epoch 190 | val MSE=0.003380 | best=0.003380\n",
      "epoch 191 | val MSE=0.003557 | best=0.003380\n",
      "epoch 192 | val MSE=0.003550 | best=0.003380\n",
      "epoch 193 | val MSE=0.003483 | best=0.003380\n",
      "epoch 194 | val MSE=0.003459 | best=0.003380\n",
      "[MLP-MATCH] epoch 195 | train MSE=0.003524 | val MSE=0.004432\n",
      "epoch 195 | val MSE=0.004432 | best=0.003380\n",
      "epoch 196 | val MSE=0.003379 | best=0.003379\n",
      "epoch 197 | val MSE=0.003544 | best=0.003379\n",
      "epoch 198 | val MSE=0.004332 | best=0.003379\n",
      "epoch 199 | val MSE=0.003484 | best=0.003379\n",
      "[MLP-MATCH] epoch 200 | train MSE=0.003538 | val MSE=0.003376\n",
      "epoch 200 | val MSE=0.003376 | best=0.003376\n",
      "epoch 201 | val MSE=0.003336 | best=0.003336\n",
      "epoch 202 | val MSE=0.003453 | best=0.003336\n",
      "epoch 203 | val MSE=0.003530 | best=0.003336\n",
      "epoch 204 | val MSE=0.003630 | best=0.003336\n",
      "[MLP-MATCH] epoch 205 | train MSE=0.003489 | val MSE=0.003468\n",
      "epoch 205 | val MSE=0.003468 | best=0.003336\n",
      "epoch 206 | val MSE=0.006532 | best=0.003336\n",
      "epoch 207 | val MSE=0.003795 | best=0.003336\n",
      "epoch 208 | val MSE=0.003471 | best=0.003336\n",
      "epoch 209 | val MSE=0.003274 | best=0.003274\n",
      "[MLP-MATCH] epoch 210 | train MSE=0.003703 | val MSE=0.003297\n",
      "epoch 210 | val MSE=0.003297 | best=0.003274\n",
      "epoch 211 | val MSE=0.003489 | best=0.003274\n",
      "epoch 212 | val MSE=0.003458 | best=0.003274\n",
      "epoch 213 | val MSE=0.003423 | best=0.003274\n",
      "epoch 214 | val MSE=0.003285 | best=0.003274\n",
      "[MLP-MATCH] epoch 215 | train MSE=0.003578 | val MSE=0.004231\n",
      "epoch 215 | val MSE=0.004231 | best=0.003274\n",
      "epoch 216 | val MSE=0.003436 | best=0.003274\n",
      "epoch 217 | val MSE=0.003619 | best=0.003274\n",
      "epoch 218 | val MSE=0.003500 | best=0.003274\n",
      "epoch 219 | val MSE=0.003253 | best=0.003253\n",
      "[MLP-MATCH] epoch 220 | train MSE=0.003370 | val MSE=0.003811\n",
      "epoch 220 | val MSE=0.003811 | best=0.003253\n",
      "epoch 221 | val MSE=0.003312 | best=0.003253\n",
      "epoch 222 | val MSE=0.003520 | best=0.003253\n",
      "epoch 223 | val MSE=0.003273 | best=0.003253\n",
      "epoch 224 | val MSE=0.003930 | best=0.003253\n",
      "[MLP-MATCH] epoch 225 | train MSE=0.003522 | val MSE=0.003519\n",
      "epoch 225 | val MSE=0.003519 | best=0.003253\n",
      "epoch 226 | val MSE=0.003298 | best=0.003253\n",
      "epoch 227 | val MSE=0.003632 | best=0.003253\n",
      "epoch 228 | val MSE=0.003400 | best=0.003253\n",
      "epoch 229 | val MSE=0.003628 | best=0.003253\n",
      "[MLP-MATCH] epoch 230 | train MSE=0.003517 | val MSE=0.003218\n",
      "epoch 230 | val MSE=0.003218 | best=0.003218\n",
      "epoch 231 | val MSE=0.003411 | best=0.003218\n",
      "epoch 232 | val MSE=0.003216 | best=0.003216\n",
      "epoch 233 | val MSE=0.003287 | best=0.003216\n",
      "epoch 234 | val MSE=0.003261 | best=0.003216\n",
      "[MLP-MATCH] epoch 235 | train MSE=0.003296 | val MSE=0.003507\n",
      "epoch 235 | val MSE=0.003507 | best=0.003216\n",
      "epoch 236 | val MSE=0.003509 | best=0.003216\n",
      "epoch 237 | val MSE=0.003673 | best=0.003216\n",
      "epoch 238 | val MSE=0.003779 | best=0.003216\n",
      "epoch 239 | val MSE=0.003426 | best=0.003216\n",
      "[MLP-MATCH] epoch 240 | train MSE=0.003411 | val MSE=0.003244\n",
      "epoch 240 | val MSE=0.003244 | best=0.003216\n",
      "epoch 241 | val MSE=0.003540 | best=0.003216\n",
      "epoch 242 | val MSE=0.003316 | best=0.003216\n",
      "epoch 243 | val MSE=0.003677 | best=0.003216\n",
      "epoch 244 | val MSE=0.003208 | best=0.003208\n",
      "[MLP-MATCH] epoch 245 | train MSE=0.003231 | val MSE=0.003193\n",
      "epoch 245 | val MSE=0.003193 | best=0.003193\n",
      "epoch 246 | val MSE=0.003549 | best=0.003193\n",
      "epoch 247 | val MSE=0.003169 | best=0.003169\n",
      "epoch 248 | val MSE=0.003188 | best=0.003169\n",
      "epoch 249 | val MSE=0.003207 | best=0.003169\n",
      "[MLP-MATCH] epoch 250 | train MSE=0.003243 | val MSE=0.003165\n",
      "epoch 250 | val MSE=0.003165 | best=0.003165\n",
      "epoch 251 | val MSE=0.003117 | best=0.003117\n",
      "epoch 252 | val MSE=0.003113 | best=0.003113\n",
      "epoch 253 | val MSE=0.003355 | best=0.003113\n",
      "epoch 254 | val MSE=0.003231 | best=0.003113\n",
      "[MLP-MATCH] epoch 255 | train MSE=0.003327 | val MSE=0.003288\n",
      "epoch 255 | val MSE=0.003288 | best=0.003113\n",
      "epoch 256 | val MSE=0.003155 | best=0.003113\n",
      "epoch 257 | val MSE=0.003203 | best=0.003113\n",
      "epoch 258 | val MSE=0.003257 | best=0.003113\n",
      "epoch 259 | val MSE=0.003423 | best=0.003113\n",
      "[MLP-MATCH] epoch 260 | train MSE=0.003275 | val MSE=0.003149\n",
      "epoch 260 | val MSE=0.003149 | best=0.003113\n",
      "epoch 261 | val MSE=0.003124 | best=0.003113\n",
      "epoch 262 | val MSE=0.003184 | best=0.003113\n",
      "epoch 263 | val MSE=0.003162 | best=0.003113\n",
      "epoch 264 | val MSE=0.003144 | best=0.003113\n",
      "[MLP-MATCH] epoch 265 | train MSE=0.003144 | val MSE=0.003121\n",
      "epoch 265 | val MSE=0.003121 | best=0.003113\n",
      "epoch 266 | val MSE=0.003163 | best=0.003113\n",
      "epoch 267 | val MSE=0.003138 | best=0.003113\n",
      "epoch 268 | val MSE=0.003101 | best=0.003101\n",
      "epoch 269 | val MSE=0.003178 | best=0.003101\n",
      "[MLP-MATCH] epoch 270 | train MSE=0.003158 | val MSE=0.003106\n",
      "epoch 270 | val MSE=0.003106 | best=0.003101\n",
      "epoch 271 | val MSE=0.003157 | best=0.003101\n",
      "epoch 272 | val MSE=0.003094 | best=0.003094\n",
      "epoch 273 | val MSE=0.003212 | best=0.003094\n",
      "epoch 274 | val MSE=0.003281 | best=0.003094\n",
      "[MLP-MATCH] epoch 275 | train MSE=0.003188 | val MSE=0.003115\n",
      "epoch 275 | val MSE=0.003115 | best=0.003094\n",
      "epoch 276 | val MSE=0.003175 | best=0.003094\n",
      "epoch 277 | val MSE=0.003118 | best=0.003094\n",
      "epoch 278 | val MSE=0.003105 | best=0.003094\n",
      "epoch 279 | val MSE=0.003336 | best=0.003094\n",
      "[MLP-MATCH] epoch 280 | train MSE=0.003185 | val MSE=0.003157\n",
      "epoch 280 | val MSE=0.003157 | best=0.003094\n",
      "epoch 281 | val MSE=0.003091 | best=0.003091\n",
      "epoch 282 | val MSE=0.003221 | best=0.003091\n",
      "epoch 283 | val MSE=0.003148 | best=0.003091\n",
      "epoch 284 | val MSE=0.003218 | best=0.003091\n",
      "[MLP-MATCH] epoch 285 | train MSE=0.003161 | val MSE=0.003094\n",
      "epoch 285 | val MSE=0.003094 | best=0.003091\n",
      "epoch 286 | val MSE=0.003100 | best=0.003091\n",
      "epoch 287 | val MSE=0.003159 | best=0.003091\n",
      "epoch 288 | val MSE=0.003147 | best=0.003091\n",
      "epoch 289 | val MSE=0.003135 | best=0.003091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLP-MATCH] epoch 290 | train MSE=0.003152 | val MSE=0.003111\n",
      "epoch 290 | val MSE=0.003111 | best=0.003091\n",
      "epoch 291 | val MSE=0.003068 | best=0.003068\n",
      "epoch 292 | val MSE=0.003186 | best=0.003068\n",
      "epoch 293 | val MSE=0.003264 | best=0.003068\n",
      "epoch 294 | val MSE=0.003197 | best=0.003068\n",
      "[MLP-MATCH] epoch 295 | train MSE=0.003144 | val MSE=0.003100\n",
      "epoch 295 | val MSE=0.003100 | best=0.003068\n",
      "epoch 296 | val MSE=0.003214 | best=0.003068\n",
      "epoch 297 | val MSE=0.003128 | best=0.003068\n",
      "epoch 298 | val MSE=0.003077 | best=0.003068\n",
      "epoch 299 | val MSE=0.003113 | best=0.003068\n",
      "[MLP-MATCH] epoch 300 | train MSE=0.003165 | val MSE=0.003199\n",
      "epoch 300 | val MSE=0.003199 | best=0.003068\n",
      "epoch 301 | val MSE=0.003104 | best=0.003068\n",
      "epoch 302 | val MSE=0.003130 | best=0.003068\n",
      "epoch 303 | val MSE=0.003079 | best=0.003068\n",
      "epoch 304 | val MSE=0.003115 | best=0.003068\n",
      "[MLP-MATCH] epoch 305 | train MSE=0.003114 | val MSE=0.003077\n",
      "epoch 305 | val MSE=0.003077 | best=0.003068\n",
      "epoch 306 | val MSE=0.003130 | best=0.003068\n",
      "epoch 307 | val MSE=0.003086 | best=0.003068\n",
      "epoch 308 | val MSE=0.003057 | best=0.003057\n",
      "epoch 309 | val MSE=0.003094 | best=0.003057\n",
      "[MLP-MATCH] epoch 310 | train MSE=0.003137 | val MSE=0.003125\n",
      "epoch 310 | val MSE=0.003125 | best=0.003057\n",
      "epoch 311 | val MSE=0.003060 | best=0.003057\n",
      "epoch 312 | val MSE=0.003085 | best=0.003057\n",
      "epoch 313 | val MSE=0.003069 | best=0.003057\n",
      "epoch 314 | val MSE=0.003070 | best=0.003057\n",
      "[MLP-MATCH] epoch 315 | train MSE=0.003111 | val MSE=0.003078\n",
      "epoch 315 | val MSE=0.003078 | best=0.003057\n",
      "epoch 316 | val MSE=0.003112 | best=0.003057\n",
      "epoch 317 | val MSE=0.003060 | best=0.003057\n",
      "epoch 318 | val MSE=0.003065 | best=0.003057\n",
      "epoch 319 | val MSE=0.003080 | best=0.003057\n",
      "[MLP-MATCH] epoch 320 | train MSE=0.003083 | val MSE=0.003067\n",
      "epoch 320 | val MSE=0.003067 | best=0.003057\n",
      "epoch 321 | val MSE=0.003044 | best=0.003044\n",
      "epoch 322 | val MSE=0.003041 | best=0.003041\n",
      "epoch 323 | val MSE=0.003049 | best=0.003041\n",
      "epoch 324 | val MSE=0.003070 | best=0.003041\n",
      "[MLP-MATCH] epoch 325 | train MSE=0.003092 | val MSE=0.003075\n",
      "epoch 325 | val MSE=0.003075 | best=0.003041\n",
      "epoch 326 | val MSE=0.003064 | best=0.003041\n",
      "epoch 327 | val MSE=0.003112 | best=0.003041\n",
      "epoch 328 | val MSE=0.003068 | best=0.003041\n",
      "epoch 329 | val MSE=0.003092 | best=0.003041\n",
      "[MLP-MATCH] epoch 330 | train MSE=0.003083 | val MSE=0.003097\n",
      "epoch 330 | val MSE=0.003097 | best=0.003041\n",
      "epoch 331 | val MSE=0.003060 | best=0.003041\n",
      "epoch 332 | val MSE=0.003065 | best=0.003041\n",
      "epoch 333 | val MSE=0.003074 | best=0.003041\n",
      "epoch 334 | val MSE=0.003070 | best=0.003041\n",
      "[MLP-MATCH] epoch 335 | train MSE=0.003063 | val MSE=0.003054\n",
      "epoch 335 | val MSE=0.003054 | best=0.003041\n",
      "epoch 336 | val MSE=0.003060 | best=0.003041\n",
      "epoch 337 | val MSE=0.003056 | best=0.003041\n",
      "epoch 338 | val MSE=0.003045 | best=0.003041\n",
      "epoch 339 | val MSE=0.003047 | best=0.003041\n",
      "[MLP-MATCH] epoch 340 | train MSE=0.003067 | val MSE=0.003041\n",
      "epoch 340 | val MSE=0.003041 | best=0.003041\n",
      "epoch 341 | val MSE=0.003050 | best=0.003041\n",
      "epoch 342 | val MSE=0.003053 | best=0.003041\n",
      "epoch 343 | val MSE=0.003037 | best=0.003037\n",
      "epoch 344 | val MSE=0.003047 | best=0.003037\n",
      "[MLP-MATCH] epoch 345 | train MSE=0.003065 | val MSE=0.003054\n",
      "epoch 345 | val MSE=0.003054 | best=0.003037\n",
      "epoch 346 | val MSE=0.003045 | best=0.003037\n",
      "epoch 347 | val MSE=0.003043 | best=0.003037\n",
      "epoch 348 | val MSE=0.003058 | best=0.003037\n",
      "epoch 349 | val MSE=0.003047 | best=0.003037\n",
      "[MLP-MATCH] epoch 350 | train MSE=0.003068 | val MSE=0.003046\n",
      "epoch 350 | val MSE=0.003046 | best=0.003037\n",
      "epoch 351 | val MSE=0.003033 | best=0.003033\n",
      "epoch 352 | val MSE=0.003043 | best=0.003033\n",
      "epoch 353 | val MSE=0.003035 | best=0.003033\n",
      "epoch 354 | val MSE=0.003042 | best=0.003033\n",
      "[MLP-MATCH] epoch 355 | train MSE=0.003069 | val MSE=0.003051\n",
      "epoch 355 | val MSE=0.003051 | best=0.003033\n",
      "epoch 356 | val MSE=0.003038 | best=0.003033\n",
      "epoch 357 | val MSE=0.003048 | best=0.003033\n",
      "epoch 358 | val MSE=0.003072 | best=0.003033\n",
      "epoch 359 | val MSE=0.003075 | best=0.003033\n",
      "[MLP-MATCH] epoch 360 | train MSE=0.003076 | val MSE=0.003060\n",
      "epoch 360 | val MSE=0.003060 | best=0.003033\n",
      "epoch 361 | val MSE=0.003033 | best=0.003033\n",
      "epoch 362 | val MSE=0.003052 | best=0.003033\n",
      "epoch 363 | val MSE=0.003068 | best=0.003033\n",
      "epoch 364 | val MSE=0.003041 | best=0.003033\n",
      "[MLP-MATCH] epoch 365 | train MSE=0.003061 | val MSE=0.003036\n",
      "epoch 365 | val MSE=0.003036 | best=0.003033\n",
      "epoch 366 | val MSE=0.003037 | best=0.003033\n",
      "epoch 367 | val MSE=0.003038 | best=0.003033\n",
      "epoch 368 | val MSE=0.003043 | best=0.003033\n",
      "epoch 369 | val MSE=0.003043 | best=0.003033\n",
      "[MLP-MATCH] epoch 370 | train MSE=0.003062 | val MSE=0.003036\n",
      "epoch 370 | val MSE=0.003036 | best=0.003033\n",
      "epoch 371 | val MSE=0.003038 | best=0.003033\n",
      "epoch 372 | val MSE=0.003034 | best=0.003033\n",
      "epoch 373 | val MSE=0.003032 | best=0.003032\n",
      "epoch 374 | val MSE=0.003037 | best=0.003032\n",
      "[MLP-MATCH] epoch 375 | train MSE=0.003056 | val MSE=0.003044\n",
      "epoch 375 | val MSE=0.003044 | best=0.003032\n",
      "epoch 376 | val MSE=0.003034 | best=0.003032\n",
      "epoch 377 | val MSE=0.003038 | best=0.003032\n",
      "epoch 378 | val MSE=0.003035 | best=0.003032\n",
      "epoch 379 | val MSE=0.003031 | best=0.003031\n",
      "[MLP-MATCH] epoch 380 | train MSE=0.003059 | val MSE=0.003042\n",
      "epoch 380 | val MSE=0.003042 | best=0.003031\n",
      "epoch 381 | val MSE=0.003040 | best=0.003031\n",
      "epoch 382 | val MSE=0.003048 | best=0.003031\n",
      "epoch 383 | val MSE=0.003064 | best=0.003031\n",
      "epoch 384 | val MSE=0.003038 | best=0.003031\n",
      "[MLP-MATCH] epoch 385 | train MSE=0.003057 | val MSE=0.003074\n",
      "epoch 385 | val MSE=0.003074 | best=0.003031\n",
      "epoch 386 | val MSE=0.003035 | best=0.003031\n",
      "epoch 387 | val MSE=0.003037 | best=0.003031\n",
      "epoch 388 | val MSE=0.003054 | best=0.003031\n",
      "epoch 389 | val MSE=0.003032 | best=0.003031\n",
      "[MLP-MATCH] epoch 390 | train MSE=0.003055 | val MSE=0.003038\n",
      "epoch 390 | val MSE=0.003038 | best=0.003031\n",
      "epoch 391 | val MSE=0.003036 | best=0.003031\n",
      "epoch 392 | val MSE=0.003033 | best=0.003031\n",
      "epoch 393 | val MSE=0.003047 | best=0.003031\n",
      "epoch 394 | val MSE=0.003032 | best=0.003031\n",
      "[MLP-MATCH] epoch 395 | train MSE=0.003054 | val MSE=0.003030\n",
      "epoch 395 | val MSE=0.003030 | best=0.003030\n",
      "epoch 396 | val MSE=0.003031 | best=0.003030\n",
      "epoch 397 | val MSE=0.003035 | best=0.003030\n",
      "epoch 398 | val MSE=0.003040 | best=0.003030\n",
      "epoch 399 | val MSE=0.003032 | best=0.003030\n",
      "[MLP-MATCH] epoch 400 | train MSE=0.003052 | val MSE=0.003038\n",
      "epoch 400 | val MSE=0.003038 | best=0.003030\n",
      "epoch 401 | val MSE=0.003032 | best=0.003030\n",
      "epoch 402 | val MSE=0.003038 | best=0.003030\n",
      "epoch 403 | val MSE=0.003033 | best=0.003030\n",
      "epoch 404 | val MSE=0.003035 | best=0.003030\n",
      "[MLP-MATCH] epoch 405 | train MSE=0.003055 | val MSE=0.003038\n",
      "epoch 405 | val MSE=0.003038 | best=0.003030\n",
      "epoch 406 | val MSE=0.003030 | best=0.003030\n",
      "epoch 407 | val MSE=0.003031 | best=0.003030\n",
      "epoch 408 | val MSE=0.003037 | best=0.003030\n",
      "epoch 409 | val MSE=0.003032 | best=0.003030\n",
      "[MLP-MATCH] epoch 410 | train MSE=0.003049 | val MSE=0.003031\n",
      "epoch 410 | val MSE=0.003031 | best=0.003030\n",
      "epoch 411 | val MSE=0.003035 | best=0.003030\n",
      "epoch 412 | val MSE=0.003030 | best=0.003030\n",
      "epoch 413 | val MSE=0.003032 | best=0.003030\n",
      "epoch 414 | val MSE=0.003032 | best=0.003030\n",
      "[MLP-MATCH] epoch 415 | train MSE=0.003048 | val MSE=0.003031\n",
      "epoch 415 | val MSE=0.003031 | best=0.003030\n",
      "epoch 416 | val MSE=0.003030 | best=0.003030\n",
      "epoch 417 | val MSE=0.003035 | best=0.003030\n",
      "epoch 418 | val MSE=0.003031 | best=0.003030\n",
      "epoch 419 | val MSE=0.003032 | best=0.003030\n",
      "[MLP-MATCH] epoch 420 | train MSE=0.003048 | val MSE=0.003032\n",
      "epoch 420 | val MSE=0.003032 | best=0.003030\n",
      "epoch 421 | val MSE=0.003032 | best=0.003030\n",
      "epoch 422 | val MSE=0.003031 | best=0.003030\n",
      "epoch 423 | val MSE=0.003030 | best=0.003030\n",
      "epoch 424 | val MSE=0.003030 | best=0.003030\n",
      "[MLP-MATCH] epoch 425 | train MSE=0.003047 | val MSE=0.003031\n",
      "epoch 425 | val MSE=0.003031 | best=0.003030\n",
      "epoch 426 | val MSE=0.003031 | best=0.003030\n",
      "epoch 427 | val MSE=0.003033 | best=0.003030\n",
      "epoch 428 | val MSE=0.003031 | best=0.003030\n",
      "epoch 429 | val MSE=0.003032 | best=0.003030\n",
      "[MLP-MATCH] epoch 430 | train MSE=0.003047 | val MSE=0.003031\n",
      "epoch 430 | val MSE=0.003031 | best=0.003030\n",
      "epoch 431 | val MSE=0.003031 | best=0.003030\n",
      "epoch 432 | val MSE=0.003031 | best=0.003030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 433 | val MSE=0.003031 | best=0.003030\n",
      "epoch 434 | val MSE=0.003031 | best=0.003030\n",
      "[MLP-MATCH] epoch 435 | train MSE=0.003047 | val MSE=0.003031\n",
      "epoch 435 | val MSE=0.003031 | best=0.003030\n",
      "epoch 436 | val MSE=0.003031 | best=0.003030\n",
      "epoch 437 | val MSE=0.003031 | best=0.003030\n",
      "epoch 438 | val MSE=0.003031 | best=0.003030\n",
      "epoch 439 | val MSE=0.003031 | best=0.003030\n",
      "[MLP-MATCH] epoch 440 | train MSE=0.003047 | val MSE=0.003031\n",
      "epoch 440 | val MSE=0.003031 | best=0.003030\n",
      "epoch 441 | val MSE=0.003031 | best=0.003030\n",
      "epoch 442 | val MSE=0.003031 | best=0.003030\n",
      "epoch 443 | val MSE=0.003031 | best=0.003030\n",
      "epoch 444 | val MSE=0.003030 | best=0.003030\n",
      "[MLP-MATCH] epoch 445 | train MSE=0.003047 | val MSE=0.003031\n",
      "epoch 445 | val MSE=0.003031 | best=0.003030\n",
      "epoch 446 | val MSE=0.003030 | best=0.003030\n",
      "epoch 447 | val MSE=0.003031 | best=0.003030\n",
      "epoch 448 | val MSE=0.003031 | best=0.003030\n",
      "epoch 449 | val MSE=0.003031 | best=0.003030\n",
      "[MLP-MATCH] epoch 450 | train MSE=0.003047 | val MSE=0.003030\n",
      "epoch 450 | val MSE=0.003030 | best=0.003030\n",
      "epoch 451 | val MSE=0.003031 | best=0.003030\n",
      "epoch 452 | val MSE=0.003031 | best=0.003030\n",
      "epoch 453 | val MSE=0.003030 | best=0.003030\n",
      "epoch 454 | val MSE=0.003031 | best=0.003030\n",
      "[MLP-MATCH] epoch 455 | train MSE=0.003047 | val MSE=0.003030\n",
      "epoch 455 | val MSE=0.003030 | best=0.003030\n",
      "epoch 456 | val MSE=0.003030 | best=0.003030\n",
      "epoch 457 | val MSE=0.003030 | best=0.003030\n",
      "epoch 458 | val MSE=0.003031 | best=0.003030\n",
      "epoch 459 | val MSE=0.003031 | best=0.003030\n",
      "[MLP-MATCH] epoch 460 | train MSE=0.003047 | val MSE=0.003030\n",
      "epoch 460 | val MSE=0.003030 | best=0.003030\n",
      "epoch 461 | val MSE=0.003030 | best=0.003030\n",
      "epoch 462 | val MSE=0.003030 | best=0.003030\n",
      "epoch 463 | val MSE=0.003031 | best=0.003030\n",
      "epoch 464 | val MSE=0.003031 | best=0.003030\n",
      "[MLP-MATCH] epoch 465 | train MSE=0.003047 | val MSE=0.003031\n",
      "epoch 465 | val MSE=0.003031 | best=0.003030\n",
      "epoch 466 | val MSE=0.003031 | best=0.003030\n",
      "epoch 467 | val MSE=0.003031 | best=0.003030\n",
      "epoch 468 | val MSE=0.003031 | best=0.003030\n",
      "epoch 469 | val MSE=0.003031 | best=0.003030\n",
      "[MLP-MATCH] epoch 470 | train MSE=0.003047 | val MSE=0.003030\n",
      "epoch 470 | val MSE=0.003030 | best=0.003030\n",
      "epoch 471 | val MSE=0.003031 | best=0.003030\n",
      "epoch 472 | val MSE=0.003031 | best=0.003030\n",
      "epoch 473 | val MSE=0.003031 | best=0.003030\n",
      "epoch 474 | val MSE=0.003031 | best=0.003030\n",
      "[MLP-MATCH] epoch 475 | train MSE=0.003047 | val MSE=0.003030\n",
      "epoch 475 | val MSE=0.003030 | best=0.003030\n",
      "epoch 476 | val MSE=0.003031 | best=0.003030\n",
      "epoch 477 | val MSE=0.003030 | best=0.003030\n",
      "epoch 478 | val MSE=0.003030 | best=0.003030\n",
      "epoch 479 | val MSE=0.003032 | best=0.003030\n",
      "[MLP-MATCH] epoch 480 | train MSE=0.003047 | val MSE=0.003031\n",
      "epoch 480 | val MSE=0.003031 | best=0.003030\n",
      "epoch 481 | val MSE=0.003031 | best=0.003030\n",
      "epoch 482 | val MSE=0.003031 | best=0.003030\n",
      "epoch 483 | val MSE=0.003030 | best=0.003030\n",
      "epoch 484 | val MSE=0.003030 | best=0.003030\n",
      "[MLP-MATCH] epoch 485 | train MSE=0.003047 | val MSE=0.003030\n",
      "epoch 485 | val MSE=0.003030 | best=0.003030\n",
      "epoch 486 | val MSE=0.003030 | best=0.003030\n",
      "epoch 487 | val MSE=0.003031 | best=0.003030\n",
      "epoch 488 | val MSE=0.003030 | best=0.003030\n",
      "epoch 489 | val MSE=0.003031 | best=0.003030\n",
      "[MLP-MATCH] epoch 490 | train MSE=0.003047 | val MSE=0.003030\n",
      "epoch 490 | val MSE=0.003030 | best=0.003030\n",
      "epoch 491 | val MSE=0.003030 | best=0.003030\n",
      "epoch 492 | val MSE=0.003030 | best=0.003030\n",
      "epoch 493 | val MSE=0.003030 | best=0.003030\n",
      "epoch 494 | val MSE=0.003030 | best=0.003030\n",
      "[MLP-MATCH] epoch 495 | train MSE=0.003047 | val MSE=0.003031\n",
      "epoch 495 | val MSE=0.003031 | best=0.003030\n",
      "epoch 496 | val MSE=0.003031 | best=0.003030\n",
      "epoch 497 | val MSE=0.003030 | best=0.003030\n",
      "epoch 498 | val MSE=0.003031 | best=0.003030\n",
      "epoch 499 | val MSE=0.003030 | best=0.003030\n",
      "[MLP-MATCH] epoch 500 | train MSE=0.003047 | val MSE=0.003031\n",
      "epoch 500 | val MSE=0.003031 | best=0.003030\n",
      "MLP train time: 137.70s | params: 5410\n"
     ]
    }
   ],
   "source": [
    "mlp, t_mlp = timed(lambda: train_mlp_matched_to_kan(\n",
    "    kan_model=kan,\n",
    "    device=device,\n",
    "    n_hidden_layers=2,\n",
    "    act=\"tanh\",\n",
    "    epochs=500,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4\n",
    "), device)\n",
    "\n",
    "\n",
    "print(f\"MLP train time: {t_mlp:.2f}s | params: {count_params(mlp)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e9b7cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def find_span(x, knots, n_basis, degree):\n",
    "    # span s.t. t[span] <= x < t[span+1]\n",
    "    span = torch.bucketize(x, knots, right=True) - 1\n",
    "    return span.clamp(min=degree, max=n_basis - 1)\n",
    "\n",
    "\n",
    "def bspline_basis_funs_sparse(x, knots, n_basis: int, degree: int):\n",
    "    \"\"\"\n",
    "    Vectorized computation of the (degree+1) non-zero B-spline basis functions at x.\n",
    "    Returns:\n",
    "      span: [B, D] spans\n",
    "      N:    [B, D, degree+1] basis values for indices (span-degree ... span)\n",
    "    x: [B, D]\n",
    "    \"\"\"\n",
    "    device = x.device\n",
    "    B, D = x.shape\n",
    "    p = degree\n",
    "\n",
    "    span = find_span(x, knots, n_basis, p)  # [B, D]\n",
    "\n",
    "    # Algorithm A2.2 (The NURBS Book) vectorized\n",
    "    N = torch.zeros((B, D, p + 1), device=device, dtype=x.dtype)\n",
    "    left = torch.zeros((B, D, p + 1), device=device, dtype=x.dtype)\n",
    "    right = torch.zeros((B, D, p + 1), device=device, dtype=x.dtype)\n",
    "\n",
    "    N[..., 0] = 1.0\n",
    "\n",
    "    # We need knot values t[span + j] and t[span + 1 - j] etc.\n",
    "    # We'll gather knots by index.\n",
    "    for j in range(1, p + 1):\n",
    "        # left[j] = x - t[span + 1 - j]\n",
    "        idx_left = (span + 1 - j).clamp(min=0, max=knots.numel() - 1)\n",
    "        t_left = knots[idx_left]  # [B, D]\n",
    "        left[..., j] = x - t_left\n",
    "\n",
    "        # right[j] = t[span + j] - x\n",
    "        idx_right = (span + j).clamp(min=0, max=knots.numel() - 1)\n",
    "        t_right = knots[idx_right]  # [B, D]\n",
    "        right[..., j] = t_right - x\n",
    "\n",
    "        saved = torch.zeros((B, D), device=device, dtype=x.dtype)\n",
    "        for r in range(0, j):\n",
    "            denom = right[..., r + 1] + left[..., j - r]\n",
    "            # safe divide\n",
    "            denom = torch.where(denom == 0, torch.ones_like(denom), denom)\n",
    "            temp = N[..., r] / denom\n",
    "            N[..., r] = saved + right[..., r + 1] * temp\n",
    "            saved = left[..., j - r] * temp\n",
    "        N[..., j] = saved\n",
    "\n",
    "    return span, N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f3dd8f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class KANSplineLayerFast(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int,\n",
    "                 n_basis: int = 24, degree: int = 3,\n",
    "                 xmin: float = -1.0, xmax: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.n_basis = n_basis\n",
    "        self.degree = degree\n",
    "        self.xmin = float(xmin)\n",
    "        self.xmax = float(xmax)\n",
    "\n",
    "        # [out, in, K]\n",
    "        self.coeff = nn.Parameter(0.01 * torch.randn(out_dim, in_dim, n_basis))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_dim))\n",
    "        self.lin_weight = nn.Parameter(torch.empty(out_dim, in_dim))\n",
    "        nn.init.kaiming_uniform_(self.lin_weight, a=math.sqrt(5))  # init standard\n",
    "\n",
    "\n",
    "        # Open-uniform knots (buffer)\n",
    "        knots = self._make_open_uniform_knots(n_basis, degree, self.xmin, self.xmax)\n",
    "        self.register_buffer(\"knots\", knots)\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_open_uniform_knots(n_basis, degree, xmin, xmax):\n",
    "        device = torch.device(\"cpu\")\n",
    "        n_knots = n_basis + degree + 1\n",
    "        n_inner = n_knots - 2 * (degree + 1)\n",
    "        if n_inner > 0:\n",
    "            inner = torch.linspace(xmin, xmax, steps=n_inner + 2, device=device)[1:-1]\n",
    "            knots = torch.cat([\n",
    "                torch.full((degree + 1,), xmin, device=device),\n",
    "                inner,\n",
    "                torch.full((degree + 1,), xmax, device=device),\n",
    "            ])\n",
    "        else:\n",
    "            knots = torch.cat([\n",
    "                torch.full((degree + 1,), xmin, device=device),\n",
    "                torch.full((degree + 1,), xmax, device=device),\n",
    "            ])\n",
    "        return knots\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        eps = 1e-7\n",
    "        x = torch.clamp(x, self.xmin, self.xmax - eps)\n",
    "        B, D = x.shape\n",
    "        p = self.degree\n",
    "\n",
    "        with torch.no_grad():\n",
    "            span, N = bspline_basis_funs_sparse(x, self.knots, self.n_basis, p)\n",
    "            k = torch.arange(p + 1, device=x.device).view(1, 1, p + 1)\n",
    "            idx = (span.unsqueeze(-1) - p + k).clamp(0, self.n_basis - 1)\n",
    "\n",
    "        coeff_exp = self.coeff.unsqueeze(0).repeat(B, 1, 1, 1).contiguous()  # [B,out,in,K]\n",
    "        idx_exp = idx.unsqueeze(1).expand(B, self.out_dim, self.in_dim, p + 1)\n",
    "        gathered = torch.gather(coeff_exp, dim=3, index=idx_exp)              # [B,out,in,p+1]\n",
    "\n",
    "        out_spline = (gathered * N.unsqueeze(1)).sum(dim=(2, 3))              # [B,out]\n",
    "        out_lin = torch.einsum(\"bi,oi->bo\", x, self.lin_weight)               # [B,out]\n",
    "\n",
    "        return out_lin + out_spline + self.bias\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "class KANFast(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, n_basis=24, degree=3, xmin=-1.0, xmax=1.0):\n",
    "        super().__init__()\n",
    "        self.l1 = KANSplineLayerFast(in_dim, hidden_dim, n_basis, degree, xmin, xmax)\n",
    "        self.l2 = KANSplineLayerFast(hidden_dim, out_dim, n_basis, degree, xmin, xmax)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.l1(x)\n",
    "        return self.l2(h)\n",
    "\n",
    "\n",
    "    def spline_smoothness_penalty(self):\n",
    "        penalty = 0.0\n",
    "        for layer in (self.l1, self.l2):\n",
    "            c = layer.coeff\n",
    "            d2 = c[..., 2:] - 2.0 * c[..., 1:-1] + c[..., :-2]\n",
    "            penalty = penalty + (d2 ** 2).mean()\n",
    "        return penalty\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "104cc508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "def train_kan_fast(\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    n=30000,\n",
    "    in_dim=6,\n",
    "    hidden_dim=32,\n",
    "    out_dim=1,\n",
    "    n_basis=24,\n",
    "    degree=3,\n",
    "    xmin=-1.0,\n",
    "    xmax=1.0,\n",
    "    noise=0.05,\n",
    "    seed=42,\n",
    "    epochs=30,\n",
    "    batch_size=4096,\n",
    "    lr=8e-4,\n",
    "    weight_decay=1e-4,\n",
    "    lambda_smooth=1e-2,\n",
    "    grad_clip=1.0,\n",
    "    use_amp=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    \n",
    "    scheduler_type=\"step\",     # \"step\" ou \"plateau\" ou None\n",
    "    step_size=80,\n",
    "    gamma=0.5,\n",
    "    plateau_factor=0.5,\n",
    "    plateau_patience=10,\n",
    "    plateau_min_lr=1e-6,\n",
    "    best_path=\"best_kan_fast.pt\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Entraîne KANFast (doit être défini dans ton fichier) + régularisation de lissage.\n",
    "    Suppose que make_dataset(...) existe déjà et renvoie (X, y) sur device.\n",
    "    Sauvegarde le meilleur modèle (val MSE) dans best_path.\n",
    "    Retourne le modèle (chargé avec le meilleur state_dict si best_path existe).\n",
    "    \"\"\"\n",
    "    assert \"KANFast\" in globals(), \"KANFast n'est pas défini dans ce notebook/fichier.\"\n",
    "    assert \"make_dataset\" in globals(), \"make_dataset n'est pas défini dans ce notebook/fichier.\"\n",
    "\n",
    "    # Data\n",
    "    X, y = make_dataset(n=n, in_dim=in_dim, noise=noise, seed=seed, device=device)\n",
    "    n_train = int(0.85 * X.shape[0])\n",
    "    X_train, y_train = X[:n_train], y[:n_train]\n",
    "    X_val, y_val = X[n_train:], y[n_train:]\n",
    "\n",
    "    # Note: tes tensors sont déjà sur GPU (device), donc num_workers/pin_memory ne changent pas grand-chose.\n",
    "    # Si tu veux profiter pleinement du DataLoader, génère le dataset sur CPU puis transfère dans la boucle.\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(X_train, y_train),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0 if device.startswith(\"cuda\") else num_workers,\n",
    "        pin_memory=False if device.startswith(\"cuda\") else pin_memory,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        TensorDataset(X_val, y_val),\n",
    "        batch_size=max(batch_size, 4096),\n",
    "        shuffle=False,\n",
    "        num_workers=0 if device.startswith(\"cuda\") else num_workers,\n",
    "        pin_memory=False if device.startswith(\"cuda\") else pin_memory,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    model = KANFast(\n",
    "        in_dim=in_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        out_dim=out_dim,\n",
    "        n_basis=n_basis,\n",
    "        degree=degree,\n",
    "        xmin=xmin,\n",
    "        xmax=xmax,\n",
    "    ).to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = None\n",
    "    if scheduler_type == \"step\":\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=step_size, gamma=gamma)\n",
    "    elif scheduler_type == \"plateau\":\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            opt, mode=\"min\", factor=plateau_factor, patience=plateau_patience,\n",
    "            min_lr=plateau_min_lr, verbose=True\n",
    "        )\n",
    "    elif scheduler_type is None or scheduler_type == \"none\":\n",
    "        scheduler = None\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scheduler_type={scheduler_type}\")\n",
    "\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=(use_amp and device.startswith(\"cuda\")))\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_mse():\n",
    "        model.eval()\n",
    "        mse_sum, n_el = 0.0, 0\n",
    "        for xb, yb in val_loader:\n",
    "            pred = model(xb)\n",
    "            mse_sum += F.mse_loss(pred, yb, reduction=\"sum\").item()\n",
    "            n_el += yb.numel()\n",
    "        return mse_sum / n_el\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        mse_sum, n_seen = 0.0, 0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\", enabled=(use_amp and device.startswith(\"cuda\"))):\n",
    "                pred = model(xb)\n",
    "                mse = F.mse_loss(pred, yb)\n",
    "                smooth = model.spline_smoothness_penalty()\n",
    "                loss = mse + lambda_smooth * smooth\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if grad_clip is not None and grad_clip > 0:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "\n",
    "            mse_sum += mse.item() * yb.shape[0]\n",
    "            n_seen += yb.shape[0]\n",
    "\n",
    "        train_mse = mse_sum / n_seen\n",
    "        val_mse = eval_mse()\n",
    "        \n",
    "        smooth_val = float(model.spline_smoothness_penalty().detach().cpu())\n",
    "        if scheduler is not None:\n",
    "            if scheduler_type == \"plateau\":\n",
    "                scheduler.step(val_mse)\n",
    "            else:  # \"step\"\n",
    "                scheduler.step()\n",
    "\n",
    "        \n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            current_lr = opt.param_groups[0][\"lr\"]\n",
    "            print(f\"... | lr={current_lr:.2e}\")\n",
    "\n",
    "        if val_mse < best_val:\n",
    "            best_val = val_mse\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == 1 or epoch == epochs:\n",
    "            print(f\"[KANFast] epoch {epoch:03d} | train MSE={train_mse:.6f} | val MSE={val_mse:.6f} | best={best_val:.6f} | smooth={smooth_val:.6f}\")\n",
    "\n",
    "    # reload best\n",
    "    if os.path.exists(best_path):\n",
    "        model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------- LANCEMENT (avec timing) ----------\n",
    "def run_kanfast_benchmark():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device.startswith(\"cuda\"):\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    kan_fast = train_kan_fast(\n",
    "        device=device,\n",
    "        epochs=30,\n",
    "        batch_size=8192,     # tu peux tester 4096 / 8192 / 16384 selon VRAM\n",
    "        lambda_smooth=1e-2,\n",
    "        use_amp=True,\n",
    "        best_path=\"best_kan_fast.pt\",\n",
    "    )\n",
    "\n",
    "    if device.startswith(\"cuda\"):\n",
    "        torch.cuda.synchronize()\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    n_params = sum(p.numel() for p in kan_fast.parameters() if p.requires_grad)\n",
    "    print(f\"KANFast train time: {t1 - t0:.2f}s | params: {n_params}\")\n",
    "    return kan_fast\n",
    "\n",
    "\n",
    "# Exemple d'appel:\n",
    "# kan_fast = run_kanfast_benchmark()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "44977bfa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 001 | train MSE=1.990232 | val MSE=1.394223 | best=1.394223 | smooth=0.001190\n",
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 005 | train MSE=0.470598 | val MSE=0.424796 | best=0.424796 | smooth=0.001194\n",
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 010 | train MSE=0.212119 | val MSE=0.191644 | best=0.191644 | smooth=0.001219\n",
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 015 | train MSE=0.106663 | val MSE=0.095650 | best=0.095650 | smooth=0.001256\n",
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 020 | train MSE=0.052422 | val MSE=0.047749 | best=0.047749 | smooth=0.001281\n",
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 025 | train MSE=0.029109 | val MSE=0.027610 | best=0.027610 | smooth=0.001282\n",
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 030 | train MSE=0.019510 | val MSE=0.018973 | best=0.018973 | smooth=0.001277\n",
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 035 | train MSE=0.014049 | val MSE=0.013891 | best=0.013891 | smooth=0.001278\n",
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 040 | train MSE=0.010790 | val MSE=0.010881 | best=0.010881 | smooth=0.001285\n",
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 045 | train MSE=0.008729 | val MSE=0.008889 | best=0.008889 | smooth=0.001292\n",
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 050 | train MSE=0.007405 | val MSE=0.007611 | best=0.007611 | smooth=0.001299\n",
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 055 | train MSE=0.006535 | val MSE=0.006764 | best=0.006764 | smooth=0.001303\n",
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 060 | train MSE=0.005912 | val MSE=0.006145 | best=0.006145 | smooth=0.001306\n",
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 065 | train MSE=0.005471 | val MSE=0.005710 | best=0.005710 | smooth=0.001312\n",
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 070 | train MSE=0.005135 | val MSE=0.005376 | best=0.005376 | smooth=0.001319\n",
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 075 | train MSE=0.004884 | val MSE=0.005105 | best=0.005105 | smooth=0.001329\n",
      "... | lr=5.00e-04\n",
      "[KANFast] epoch 080 | train MSE=0.004659 | val MSE=0.004887 | best=0.004887 | smooth=0.001339\n",
      "... | lr=5.00e-04\n",
      "[KANFast] epoch 085 | train MSE=0.004550 | val MSE=0.004798 | best=0.004798 | smooth=0.001343\n",
      "... | lr=5.00e-04\n",
      "[KANFast] epoch 090 | train MSE=0.004466 | val MSE=0.004707 | best=0.004707 | smooth=0.001348\n",
      "... | lr=5.00e-04\n",
      "[KANFast] epoch 095 | train MSE=0.004392 | val MSE=0.004644 | best=0.004644 | smooth=0.001352\n",
      "... | lr=5.00e-04\n",
      "[KANFast] epoch 100 | train MSE=0.004320 | val MSE=0.004572 | best=0.004572 | smooth=0.001356\n",
      "... | lr=5.00e-04\n",
      "[KANFast] epoch 105 | train MSE=0.004255 | val MSE=0.004511 | best=0.004511 | smooth=0.001361\n",
      "... | lr=5.00e-04\n",
      "[KANFast] epoch 110 | train MSE=0.004190 | val MSE=0.004457 | best=0.004456 | smooth=0.001368\n",
      "... | lr=5.00e-04\n",
      "[KANFast] epoch 115 | train MSE=0.004135 | val MSE=0.004389 | best=0.004389 | smooth=0.001376\n",
      "... | lr=5.00e-04\n",
      "[KANFast] epoch 120 | train MSE=0.004082 | val MSE=0.004347 | best=0.004347 | smooth=0.001381\n",
      "... | lr=5.00e-04\n",
      "[KANFast] epoch 125 | train MSE=0.004029 | val MSE=0.004304 | best=0.004304 | smooth=0.001387\n",
      "... | lr=5.00e-04\n",
      "[KANFast] epoch 130 | train MSE=0.003988 | val MSE=0.004255 | best=0.004255 | smooth=0.001394\n",
      "... | lr=5.00e-04\n",
      "[KANFast] epoch 135 | train MSE=0.003940 | val MSE=0.004229 | best=0.004227 | smooth=0.001398\n",
      "... | lr=5.00e-04\n",
      "[KANFast] epoch 140 | train MSE=0.003897 | val MSE=0.004180 | best=0.004180 | smooth=0.001405\n",
      "... | lr=5.00e-04\n",
      "[KANFast] epoch 145 | train MSE=0.003859 | val MSE=0.004140 | best=0.004140 | smooth=0.001411\n",
      "... | lr=5.00e-04\n",
      "[KANFast] epoch 150 | train MSE=0.003818 | val MSE=0.004117 | best=0.004109 | smooth=0.001418\n",
      "... | lr=5.00e-04\n",
      "[KANFast] epoch 155 | train MSE=0.003781 | val MSE=0.004083 | best=0.004083 | smooth=0.001423\n",
      "... | lr=2.50e-04\n",
      "[KANFast] epoch 160 | train MSE=0.003752 | val MSE=0.004046 | best=0.004046 | smooth=0.001430\n",
      "... | lr=2.50e-04\n",
      "[KANFast] epoch 165 | train MSE=0.003727 | val MSE=0.004030 | best=0.004030 | smooth=0.001434\n",
      "... | lr=2.50e-04\n",
      "[KANFast] epoch 170 | train MSE=0.003710 | val MSE=0.004019 | best=0.004019 | smooth=0.001437\n",
      "... | lr=2.50e-04\n",
      "[KANFast] epoch 175 | train MSE=0.003693 | val MSE=0.004008 | best=0.004006 | smooth=0.001441\n",
      "... | lr=2.50e-04\n",
      "[KANFast] epoch 180 | train MSE=0.003679 | val MSE=0.003995 | best=0.003995 | smooth=0.001445\n",
      "... | lr=2.50e-04\n",
      "[KANFast] epoch 185 | train MSE=0.003664 | val MSE=0.003982 | best=0.003980 | smooth=0.001449\n",
      "... | lr=2.50e-04\n",
      "[KANFast] epoch 190 | train MSE=0.003650 | val MSE=0.003986 | best=0.003977 | smooth=0.001453\n",
      "... | lr=2.50e-04\n",
      "[KANFast] epoch 195 | train MSE=0.003633 | val MSE=0.003966 | best=0.003964 | smooth=0.001457\n",
      "... | lr=2.50e-04\n",
      "[KANFast] epoch 200 | train MSE=0.003618 | val MSE=0.003950 | best=0.003950 | smooth=0.001461\n",
      "... | lr=2.50e-04\n",
      "[KANFast] epoch 205 | train MSE=0.003601 | val MSE=0.003929 | best=0.003929 | smooth=0.001465\n",
      "... | lr=2.50e-04\n",
      "[KANFast] epoch 210 | train MSE=0.003588 | val MSE=0.003912 | best=0.003912 | smooth=0.001468\n",
      "... | lr=2.50e-04\n",
      "[KANFast] epoch 215 | train MSE=0.003574 | val MSE=0.003906 | best=0.003906 | smooth=0.001472\n",
      "... | lr=2.50e-04\n",
      "[KANFast] epoch 220 | train MSE=0.003561 | val MSE=0.003891 | best=0.003891 | smooth=0.001476\n",
      "... | lr=2.50e-04\n",
      "[KANFast] epoch 225 | train MSE=0.003548 | val MSE=0.003881 | best=0.003881 | smooth=0.001479\n",
      "... | lr=2.50e-04\n",
      "[KANFast] epoch 230 | train MSE=0.003533 | val MSE=0.003867 | best=0.003867 | smooth=0.001483\n",
      "... | lr=2.50e-04\n",
      "[KANFast] epoch 235 | train MSE=0.003526 | val MSE=0.003863 | best=0.003863 | smooth=0.001488\n",
      "... | lr=1.25e-04\n",
      "[KANFast] epoch 240 | train MSE=0.003509 | val MSE=0.003850 | best=0.003850 | smooth=0.001493\n",
      "... | lr=1.25e-04\n",
      "[KANFast] epoch 245 | train MSE=0.003498 | val MSE=0.003848 | best=0.003848 | smooth=0.001496\n",
      "... | lr=1.25e-04\n",
      "[KANFast] epoch 250 | train MSE=0.003491 | val MSE=0.003841 | best=0.003841 | smooth=0.001499\n",
      "... | lr=1.25e-04\n",
      "[KANFast] epoch 255 | train MSE=0.003486 | val MSE=0.003828 | best=0.003827 | smooth=0.001502\n",
      "... | lr=1.25e-04\n",
      "[KANFast] epoch 260 | train MSE=0.003478 | val MSE=0.003824 | best=0.003824 | smooth=0.001504\n",
      "... | lr=1.25e-04\n",
      "[KANFast] epoch 265 | train MSE=0.003473 | val MSE=0.003821 | best=0.003818 | smooth=0.001507\n",
      "... | lr=1.25e-04\n",
      "[KANFast] epoch 270 | train MSE=0.003467 | val MSE=0.003820 | best=0.003817 | smooth=0.001510\n",
      "... | lr=1.25e-04\n",
      "[KANFast] epoch 275 | train MSE=0.003460 | val MSE=0.003803 | best=0.003803 | smooth=0.001513\n",
      "... | lr=1.25e-04\n",
      "[KANFast] epoch 280 | train MSE=0.003454 | val MSE=0.003806 | best=0.003802 | smooth=0.001516\n",
      "... | lr=1.25e-04\n",
      "[KANFast] epoch 285 | train MSE=0.003446 | val MSE=0.003790 | best=0.003790 | smooth=0.001519\n",
      "... | lr=1.25e-04\n",
      "[KANFast] epoch 290 | train MSE=0.003440 | val MSE=0.003790 | best=0.003787 | smooth=0.001522\n",
      "... | lr=1.25e-04\n",
      "[KANFast] epoch 295 | train MSE=0.003434 | val MSE=0.003784 | best=0.003780 | smooth=0.001525\n",
      "... | lr=1.25e-04\n",
      "[KANFast] epoch 300 | train MSE=0.003428 | val MSE=0.003771 | best=0.003771 | smooth=0.001529\n"
     ]
    }
   ],
   "source": [
    "kan_fast = train_kan_fast(\n",
    "    hidden_dim=64,\n",
    "    batch_size=8192,\n",
    "    use_amp=True,\n",
    "    lr=1e-3,\n",
    "    lambda_smooth=1e-6,\n",
    "    epochs=300,\n",
    "    scheduler_type=\"step\",\n",
    "    step_size=80,\n",
    "    gamma=0.5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c219f999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 001 | train MSE=1.990242 | val MSE=1.394222 | best=1.394222 | smooth=0.001190\n",
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 005 | train MSE=0.470602 | val MSE=0.424794 | best=0.424794 | smooth=0.001194\n",
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 010 | train MSE=0.212114 | val MSE=0.191641 | best=0.191641 | smooth=0.001220\n",
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 015 | train MSE=0.106657 | val MSE=0.095646 | best=0.095646 | smooth=0.001257\n",
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 020 | train MSE=0.052417 | val MSE=0.047745 | best=0.047745 | smooth=0.001283\n",
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 025 | train MSE=0.029106 | val MSE=0.027608 | best=0.027608 | smooth=0.001284\n",
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 030 | train MSE=0.019507 | val MSE=0.018971 | best=0.018971 | smooth=0.001280\n",
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 035 | train MSE=0.014047 | val MSE=0.013890 | best=0.013890 | smooth=0.001282\n",
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 040 | train MSE=0.010788 | val MSE=0.010880 | best=0.010880 | smooth=0.001290\n",
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 045 | train MSE=0.008728 | val MSE=0.008888 | best=0.008888 | smooth=0.001297\n",
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 050 | train MSE=0.007404 | val MSE=0.007611 | best=0.007611 | smooth=0.001305\n",
      "... | lr=1.00e-03\n",
      "[KANFast] epoch 055 | train MSE=0.006534 | val MSE=0.006763 | best=0.006763 | smooth=0.001310\n",
      "... | lr=3.00e-04\n",
      "[KANFast] epoch 060 | train MSE=0.005912 | val MSE=0.006144 | best=0.006144 | smooth=0.001314\n",
      "... | lr=3.00e-04\n",
      "[KANFast] epoch 065 | train MSE=0.005711 | val MSE=0.005998 | best=0.005998 | smooth=0.001316\n",
      "... | lr=3.00e-04\n",
      "[KANFast] epoch 070 | train MSE=0.005580 | val MSE=0.005861 | best=0.005861 | smooth=0.001317\n",
      "... | lr=3.00e-04\n",
      "[KANFast] epoch 075 | train MSE=0.005454 | val MSE=0.005730 | best=0.005730 | smooth=0.001320\n",
      "... | lr=3.00e-04\n",
      "[KANFast] epoch 080 | train MSE=0.005341 | val MSE=0.005617 | best=0.005617 | smooth=0.001322\n",
      "... | lr=3.00e-04\n",
      "[KANFast] epoch 085 | train MSE=0.005230 | val MSE=0.005497 | best=0.005497 | smooth=0.001324\n",
      "... | lr=3.00e-04\n",
      "[KANFast] epoch 090 | train MSE=0.005131 | val MSE=0.005382 | best=0.005382 | smooth=0.001326\n",
      "... | lr=3.00e-04\n",
      "[KANFast] epoch 095 | train MSE=0.005037 | val MSE=0.005300 | best=0.005300 | smooth=0.001329\n",
      "... | lr=3.00e-04\n",
      "[KANFast] epoch 100 | train MSE=0.004942 | val MSE=0.005205 | best=0.005205 | smooth=0.001330\n",
      "... | lr=3.00e-04\n",
      "[KANFast] epoch 105 | train MSE=0.004856 | val MSE=0.005113 | best=0.005113 | smooth=0.001333\n",
      "... | lr=3.00e-04\n",
      "[KANFast] epoch 110 | train MSE=0.004774 | val MSE=0.005025 | best=0.005025 | smooth=0.001337\n",
      "... | lr=3.00e-04\n",
      "[KANFast] epoch 115 | train MSE=0.004703 | val MSE=0.004946 | best=0.004946 | smooth=0.001341\n",
      "... | lr=9.00e-05\n",
      "[KANFast] epoch 120 | train MSE=0.004631 | val MSE=0.004885 | best=0.004885 | smooth=0.001344\n"
     ]
    }
   ],
   "source": [
    "kan_fast = train_kan_fast(\n",
    "    batch_size=8192,\n",
    "    use_amp=False,\n",
    "    lr=1e-3,\n",
    "    lambda_smooth=0.0,\n",
    "    hidden_dim=64,\n",
    "    epochs=120\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6bd3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kan = KAN(...)\n",
    "kan.load_state_dict(torch.load(\"kan_model.pt\"))\n",
    "\n",
    "mlp = FlexibleMLP(...)\n",
    "mlp.load_state_dict(torch.load(\"mlp_model.pt\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
